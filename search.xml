<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Bitcoin Price Factor Analysis</title>
    <url>/2020/07/06/Bitcoin-Price-Factor-Analysis/</url>
    <content><![CDATA[<blockquote>
<p>Bitcoin analysis for fun.Typical time-series data analysis for reference.</p>
</blockquote>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>　This paper mainly discusses the factors in the market that are going to influence the Bitcoin price. The Bitcoin price formula is derived from the Barro(1979) model which also provides the concept of selecting the market factors in the formula. Since Bitcoin is not a traditional currency, it is not appropriate to use the law of supply and demand to determine its value. By adding some other market factors, the formula will become more theoretically feasible. Time-series analytic mechanism will be applied in this project</p>
<a id="more"></a>

<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>　The booming cryptocurrency of recent years has always caught people’s attention. Various digital coins have emerged in the market and Bitcoin has become one of the most prominent of them because of its dramatic price rising and price volatility. Yet while many people are still confused about what is the Bitcoin, where does it come from and how does it work, Bitcoin has already shaken the financial world to its core.<br>The rising of Bitcoin is quite long and winding. In late 2008, a person or group whose name Satoshi Nakamoto released a white paper called Bitcoin – A Peer to Peer Electronic Cash system to explain the idea of cryptocurrency - It is a type of decentralized digital currency without a central bank or single administrator that can be sent from user to user. In 2010, the Bitcoin was first used to trade in the real world but there are still very few people know about it at that time. However, the price volatility of Bitcoin doom to make investors suffer from the big fall. Shortly after Bitcoin reached 1000 dollars in November 2013, the price rapidly dropped to around 300 dollars. Ever since then, the price has begun its long-run rising journey. The prosperous period of cryptocurrency is in 2017 when almost the entire cryptocurrency market reached its all-time high in December 2017. </p>
<p>　Since Bitcoin is not issued by a government or certain bank, the macro-economic factor such as interest rate parity is not able to affect Bitcoin price, which means some standard economic theories cannot use to explain the formula of Bitcoin. <strong>Dyhrberg, A. H. (2016)</strong>, <strong>Buchholz, (2012)</strong> address that Bitcoin has shared some similar characteristics with gold and its price can be determined by the law of supply and demand in large extent. Because Bitcoin has such high price volatility, it is not enough by just applying supply and demand formula to the Bitcoin. There must exist some other factors that determine Bitcoin’s price. <strong>Barro (1979)</strong> has illustrated the way to determine the gold standard by using the law of supply and demand. Base on the Barro’s model (1979), we can derive a new estimable model by adding another market factor such as news post number on the Journal website. We apply the time-series mechanism to analyze the daily data we have selected from January 01, 2012 to January 01, 2019 and come up with the conclusion about the factors that do have a significant impact on Bitcoin price. Our study will give other people or investors an insight into the way that how those factors affect Bitcoin price, which will give them more references and help them make decisions while they are investing Bitcoin.</p>
<h3 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h3><p>　In 1979, Barro published the outstanding paper <strong><em>Money and The Price Level Under the Gold Standard</em></strong>. The paper specifies how price level is being evaluated in commodity goods. The level of price is determinate by the analyses based on supply and demand. The outcome of these analyses will convert gold into money currency, and gold production. Though the price of gold will fluctuate, it sets a fixed price for the commodity. The fixed price is determined by the quantity of the commodity in fiat currency. In order to come up with the absolute price level, many aspects were accounted for such as its major elements. Based on the concept of this paper, we can assume the price of Bitcoin will be affected by its supply and demand. Moreover, the supply includes the total stock of Bitcoin in circulation and exchange rate of Bitcoin, and the demand includes the size of the Bitcoin economy and the velocity of Bitcoin.</p>
<p><strong><em>Multivariate Time Series Analysis</em></strong>, written by <strong>Ruey S. Tsay</strong>, summarizes the basic concepts and ideas of multivariate time series, gives econometric models and statistical models for describing the dynamic relationship between variables, discusses the discernibility problems that arise when the model is too flexible and introduces the search for hidden The method of simplifying the structure in multidimensional time series emphasizes the applicability and limitations of the multivariate time series method. It first gives some basic concepts of multivariate time series including evaluation and quantification of time and cross-section dependencies. As the data dimension increases, the difficulty of presenting multivariate data is also significantly increased. Then it introduces the vector autoregression model which is most widely used in the multi-time series analysis. Then it introduces unit root non-stationarity and cointegration. It includes the basic theory of understanding the unit root time series and some related applications.</p>
<h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p>　As mentioned above, the concept of Bitcoin formula is based on Barro’s model. Then, the equilibrium Bitcoin price formula can be addressed as follow:<br>The supply of Bitcoin is fixed and determines the units of Bitcoin circulated in the market. Let <strong>𝐵𝑠</strong> represent the total money supply of Bitcoin, <strong>P</strong> represents Bitcoin price, and <strong>T</strong> represents the total stock of Bitcoin. We will have total money supply formula: $$𝐵^𝑠 = 𝑃𝑇$$<br>The demand of Bitcoin, <strong>𝐵𝑑</strong>, can be mainly determined by the size of Bitcoin economy, <strong>E</strong>, the days that user holding the Bitcoin, in other words, the frequency of per Bitcoin circulated in the market (days destroyed), <strong>F</strong>, and the general price level, <strong>𝑃𝑖</strong>. $$𝐵^𝑑 = \frac{𝑃𝑖𝐸}{𝐹}$$<br>　<br> The scale of the Bitcoin economy can be determined by daily trading activities. Thus, the number of Bitcoin transactions and addresses are what we use as our variables. For general price level <strong>𝑃𝑖</strong>, we apply the exchange rate between the U.S dollar and European pound as an index in the formula because our study only focuses on the U.S market. The main currency that purchases Bitcoin is U.S dollar, and if there is any appreciation or depreciation of U.S dollar against European pound; the purchasing power of U.S dollar against Bitcoin would be affected.</p>
<p>Combine the supply and demand formula we have equilibrium price formula: $$𝑃= \frac{𝑃𝑖𝐸}{𝐹𝑇}$$<br>we can rewrite the equilibrium formula as an empirical model to investigate the relationship between each variable:</p>
<p>$$𝑃_𝑡=𝛽_0+ 𝛽1𝑃𝑖_𝑡+𝛽_2𝐸_𝑡+𝛽_3𝐹_𝑡+𝛽_4𝑇_𝑡+𝜀$$<br>Bitcoin is not like any other traditional currency that controlled or issued by the government. It is not well-known to the public. The more exposure of Bitcoin, the more people will be attracted to the Bitcoin. Furthermore, if people can easily access the information they need, the search cost will be significantly reduced, which will possibly increase the investment opportunity of Bitcoin, hence increase Bitcoin price. Therefrom, we can assume the media is one of the most important factors that can influence Bitcoin price since it can easily catch people’s attention and influence people’s investment decisions. In order to make the model in line with the real world, we add one more factor that would possibly affect Bitcoin price. Here we add the Wall Street Journal’s daily news post number:<br>$$𝑃_𝑡=𝛽_0+ 𝛽1𝑃𝑖_𝑡+𝛽_2𝐸_𝑡+𝛽_3𝐹_𝑡+𝛽_4𝑇_𝑡+𝛽_5𝑁𝑒𝑤𝑠_𝑡+𝜀$$</p>
<p>　The mechanisms we use in this project are pretty straight forward. Consider the situation that variables in the formula may present the issue of endogeneity, we will use time-series analytic tools to test the properties of data step by step. Then we can figure out how those factors affect Bitcoin price by using the VAR and VEC model.</p>
<p>　In the first step, we need to do the stationary test before we do the regression, otherwise, the data will lead to spurious regression. We will use the <strong>Augmented Dickey-Fuller (ADF)</strong> to test the stationarity of all data. After the test, we can decide whether we should differentiate our data. By observing the price of Bitcoin in the whole period, we found that there are some structural breaks, which might lead to huge forecasting errors and unreliability of the model in general. And we can use the <strong>Zivot-Andrews (ZA) test</strong> to determine the accurate position of structure break statistically.</p>
<p>　In the second step, we apply the <strong>Johansen cointegration test</strong> to determine the cointegration relationship. Cointegration test allows us to describe the stationary relationship between two or more series. For each sequence alone, it may be non-stationary. The moments of these sequences, such as mean, variance or covariance, change over time, while the linear combination of these time series may have properties that do not change with time. These linear combinations could have a stable long-term relationship.</p>
<p>　After doing the test above, we can decide whether we should use VAR or VEC model in our project.Vector autoregression is a stochastic process model used to capture the linear interdependencies among multiple time series. VAR model turns every system internal variable into a lagged variable to create the model.<br> $$\Delta{y_t} =\sum_{i=1}^{p−i}\Phi_{𝑖}𝑦_{𝑡−𝑖}+𝜀_𝑡，𝑡=1,2,…,𝑇$$</p>
<p>　In this equation, $𝑦_𝑡$ is K dimension variable, P is lagged order, T is sample size, K*K dimension matrix is a coefficient to be estimated. The goal of the cointegration test is to determine the cointegration relationship in multiple time series.</p>
<p>　Consider we have a VAR(p) model with tendency term, $$y_t=c(t)+ \sum_{i=1}^{p}\Gamma_{i}𝑦_{t−i}+ 𝜀_𝑡$$</p>
<p>$c(t)=+c_0 + c_1t$, $c_i$ is a constant term. This VAR(p) model can be rewritten as ECM as follow:<br>$$\Delta{y_t} =c(t)+c_0 + c_1t+ \Pi_{y_{t-i}}+ \sum_{i=1}^{p}\Gamma_{i}^\ast\nabla𝑦_{𝑡−𝑖}+𝜀_𝑡$$</p>
<p>Let the order of matrix $\Pi$ be m, we have two situations:</p>
<p>1) Rank($\Pi$)=0, means none cointegration vectors. In this situation, $𝑦_𝑡$ have k unit roots. It becomes a VAR($p-1$) model.<br>2) Rank($\Pi$)=𝑚&gt;0, In this situation, $𝑦_𝑡$ have m cointegration vectors and k-m unit roots. So, there are k*m non-singular matrixs 𝛼 and 𝛽,<br>$$\Pi=\alpha\beta’$$</p>
<p>Vectors 𝜔𝑡=𝛽′$𝑦_𝑡$ is a I(0) process, called cointegration sequence, and 𝛼 represents how the cointegration sequence influence $\Delta{y_t}$.<br>Putting the cointegration test and ECM together, we have a VEC model. This model can be regarded as a VAR model with cointegrated restriction.</p>
<p>$$\Delta{y_t} =\alpha\beta{y_{t-1}} + \sum_{i=1}^{p−i}\Gamma_{𝑖}\Delta{𝑦_{𝑡−𝑖}}+𝜀_𝑡，𝑡=1,2,…,𝑇$$<br>It includes the regression speed of Alpha that diverged from long term equilibrium. Beta is error correction represents the long-term relationship. The coefficients of the differential terms reflect their short-term relationship. We can delete the insignificant lagged differential term.</p>
<p> In our article, we will calculate the β𝑦𝑡−1 which is error correction term 𝑒𝑐𝑚𝑡−1 by using the Johansen cointegration test and calculate all other parameters $\Gamma_i$ by applying the VEC algorithm.<br>Finally, we analyze matrix β to know the long-run relationship between Bitcoin price and other variables and then use variance decomposition to analyze the short-run effect and the proportion of each variable in the regression.</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Augmented-Dickey-Fuller-Test-for-unit-root"><a href="#Augmented-Dickey-Fuller-Test-for-unit-root" class="headerlink" title="Augmented Dickey-Fuller Test for unit root"></a>Augmented Dickey-Fuller Test for unit root</h4><p><img src="/images/pasted-50.png" alt="upload successful"><br>　According to the price chart of Bitcoin above, we can see that the price trend in one period is obviously different from the others, and we build a hypothesis that there is a structural break in this period. From the plot, we estimate the structural break begins at 2017/04/01. Then we use the Augmented Dickey-Fuller (ADF) Test for unit root and split the data into three sections: the first section is the whole period; the second section is from 2012/01/01 to 2017/11/16; the third section is from 2017/11/16 to 2019/01/01.</p>
<p><img src="/images/pasted-51.png" alt="upload successful"><br>　All the p-values are quite large, which means that the data is not stationary. Then we need to perform a differential treatment and then observe the stationarity of the data further.</p>
<p><img src="/images/pasted-52.png" alt="upload successful"><br>　After the first derivative, the p-values are significant, which means the data are basically stationary so that we can conduct further study on the data.</p>
<h4 id="Zivot-Andrews-Unit-Root-Test"><a href="#Zivot-Andrews-Unit-Root-Test" class="headerlink" title="Zivot-Andrews Unit-Root Test"></a>Zivot-Andrews Unit-Root Test</h4><p>　Through the Zivot-Andrews Test, we can statistically find the exact date of the structural break so that we can split the data into two periods to do analysis. We can observe two clearly different periods of Bitcoin market: before 2017/11/16 and after this breakpoint.</p>
<p><img src="/images/pasted-53.png" alt="upload successful"></p>
<h4 id="Johansen-Cointegration-Test"><a href="#Johansen-Cointegration-Test" class="headerlink" title="Johansen Cointegration Test"></a>Johansen Cointegration Test</h4><p>　Through the unit root test above, we can know that there are seven groups of data that are integrated of the same order I(1). Then we will use these groups of data to find the long-term equilibrium relationship between each other and use the cointegration test to judge the regression model in the next steps. For the data without cointegration, we use the VAR model for regression. For the data with cointegration, we use the VEC model for regression.In the Johansen test, we also need to determine the lag values, which we can obtain by using the AIC criterion. Since there are trace and eigenvalue two types of cointegration test, by comparing the statistical numbers of Johansen test with the critical value given in 1% critical value, we can obtain the number of cointegration relationship in each data combination.</p>
<p><img src="/images/pasted-55.png" alt="upload successful"></p>
<h4 id="Vector-Error-Correction-VEC-Model"><a href="#Vector-Error-Correction-VEC-Model" class="headerlink" title="Vector Error Correction (VEC) Model"></a>Vector Error Correction (VEC) Model</h4><p>　This model will help us find the long-term equilibrium relationship among those data combinations that have cointegration relationships. Through the above tests, we can know that the data are with cointegration. Therefore, we use the VEC model for regression to find out the long-term equilibrium relationship between combination groups with cointegration.</p>
<p><img src="/images/pasted-54.png" alt="upload successful"><br>　We can know from the result that we obtain from VEC: regardless of Period 1 or Period 2, the number of Bitcoin has a little long-term impact on Bitcoin price. In Period 1, the number of transactions has a negative impact on Bitcoin price in the long run, which means that when the number of transactions increases, the Bitcoin price drops. Since the number of addresses and that of transactions represent the same kind of influence, they should have the same influence on the Bitcoin price in the data. In terms of two periods, the number of transactions always has a negative impact, and the impact of the number of addresses is not significant. For the exchange rate, the performance in Period 1 is not the same as our previous assumption. While the exchange rate increases, the Bitcoin price decreases, which means it has a negative influence. In Period 2, the influence of exchange rate becomes positive. On the other side, the Bitcoin price in the second stage can be reflected as unreasonable and deviates from the normal value. For the days destroyed, thigs become the opposite: in Period 1, days destroyed has a positive impact on the Bitcoin price. The number of news posts is an indicator used to measure the trends of investors’ behavior in the market. In the hypothesis, the more news posts about Bitcoin, the more investors pay close attention to Bitcoin, which means the number of news posts should have a positive impact on Bitcoin price. From the long-term relationship, we can see that this effect does exist.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>　In the long run, the number of Bitcoin has no significant impact on Bitcoin price while other variables do. The exchange rate and days destroyed have the influence as our expectations. One has a positive impact and the other has a negative impact. The number of news posts always has a positive impact on Bitcoin price. In the short run, the impacts of all variables are all trivial. The factors have lag impacts on Bitcoin price. They will continuously incline over time, except the number of news posts which will reach its highest point, then decline.</p>
]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Statistic</tag>
        <tag>Time-Series</tag>
        <tag>Rstudio</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode Problem Set</title>
    <url>/2020/07/02/Leetcode-Problem-Set/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Code</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Big Data Specialization -- Introduction to Big Data</title>
    <url>/2020/06/04/Big%20Data/</url>
    <content><![CDATA[<h3 id="Course-1-Introduction-to-Big-Data"><a href="#Course-1-Introduction-to-Big-Data" class="headerlink" title="Course 1 Introduction to Big Data"></a>Course 1 Introduction to Big Data</h3><h4 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h4><ul>
<li><p><strong>In-Situ analytical processing:</strong> Bringing the computation to where data is located.</p>
</li>
<li><p><strong>SCADA(Supervisory Control and Data Acquisition)</strong><br>　　It is a type of industrial control system for remote monitoring and control of industrial processes that exists in the physical world, potentially including multiple sites, many types of sensors.In addition to monitoring and control, SCADA system can be used to define actions for reduced waste and improved efficiency in industrial processes,public or private infrastructure processes,facility processes.</p>
<a id="more"></a>

</li>
</ul>
<h4 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h4><h5 id="1-The-Characteristics-of-Big-Data-The-Six-Fundamentals-of-Big-Data"><a href="#1-The-Characteristics-of-Big-Data-The-Six-Fundamentals-of-Big-Data" class="headerlink" title="1. The Characteristics of Big Data: The Six Fundamentals of Big Data"></a><strong>1. The Characteristics of Big Data: The Six Fundamentals of Big Data</strong></h5><ol>
<li><p><strong>Volume:</strong> It’s the dimension of big data related to its size and<br>its exponential growth.</p>
</li>
<li><p><strong>Variety:</strong> refers to the ever-increasing different forms that data can come in such as text, images, voice, and geospatial data.</p>
</li>
<li><p><strong>Velocity:</strong> refers to the increasing speed at which big data is created and the increasing speed at which the data needs to be stored and analyzed.</p>
<ul>
<li><strong>Batch Processing （Slow）:</strong> Collect Data, Clean Data, Feed-In Chunks, Wait, Act. </li>
<li><strong>Real-Time Processing ( Fast):</strong> Instantly Capture Streaming Data, Feed real-time to machines, Process Real-Time, Act.</li>
</ul>
</li>
<li><p><strong>Veracity:</strong> refers to the quality of big data. It sometimes gets referred to as validity or volatility referring to the lifetime of the data.</p>
</li>
<li><p><strong>Valence (connectedness):</strong> The more connected data is, the higher it’s valences</p>
</li>
<li><p><strong>Value:</strong> The ultimate goal of big data is to get value out to the data.</p>
</li>
</ol>
<h5 id="2-Defining-the-Questions-Building-a-big-data-strategy"><a href="#2-Defining-the-Questions-Building-a-big-data-strategy" class="headerlink" title="2. Defining the Questions - Building a big data strategy:"></a><strong>2. Defining the Questions - Building a big data strategy:</strong></h5><p>As a summary, when building a big data strategy,it is important to </p>
<ul>
<li>Integrate big data analytics with business objectives</li>
<li>Communicate goals and provide organizational buy-in (Commitment, Sponsorship, Communication) for analytics projects</li>
<li>Build teams with diverse talents, and establish a teamwork mindset.</li>
<li>Remove barriers to data access and integration</li>
<li>Finally, these activities need to be iterated to respond to new business goals and technological advances</li>
</ul>
<p><strong>Five P’s of Data Science</strong></p>
<p><img src="/images/fivep.png" alt="Hadoop Ecosystem"></p>
<ol>
<li><p><strong>Purpose:</strong> The purpose refers to the challenge or set of challenges defined by your big data strategy. The purpose can be related to a scientific analysis with a hypothesis or a business metric that needs to be analyzed based often on Big Data.</p>
</li>
<li><p><strong>People:</strong> The data scientists are often seen as people who possess skills on a variety of topics including science or business domain knowledge; analysis using statistics, machine learning and mathematical knowledge; data management, programming, and computing. In practice, this is generally a group of researchers comprised of people with complementary skills.</p>
</li>
<li><p><strong>Process:</strong> The process of data science includes techniques for statistics, machine learning, programming, computing, and data management. A process is conceptual in the beginning and defines the course set of steps and how everyone can contribute to it. Note that similar reusable processes can apply to many applications with different purposes when employed within different workflows. Data science workflows combine such steps in executable graphs. We believe that process-oriented thinking is a transformative way of conducting data science to connect people and techniques to applications. Execution of such a data science process requires access to many datasets, Big and small, bringing new opportunities and challenges to Data Science. There are many Data Science steps or tasks, such as Data Collection, Data Cleaning, Data Processing/Analysis, Result Visualization, resulting in a Data Science Workflow. Data Science Processes may need user interaction and other manual operations, or be fully automated. Challenges for the data science process include 1) how to easily integrate all needed tasks to build such a process; 2) how to find the best computing resources and efficiently schedule process executions to the resources based on process definition, parameter settings, and user preferences.</p>
</li>
<li><p><strong>Platforms:</strong> Based on the needs of an application-driven purpose and the amount of data and computing required to perform this application, different computing and data platforms can be used as a part of the data science process. This scalability should be made part of any data science solution architecture.</p>
</li>
<li><p><strong>Programmability:</strong> Capturing a scalable data science process requires aid from programming languages, e.g., R, and patterns, e.g., MapReduce. Tools that provide access to such programming techniques are key to making the data science process programmable on a variety of platforms.</p>
</li>
</ol>
<h5 id="3-The-Process-of-Data-Analysis-Five-steps-activities-of-the-data-science-process"><a href="#3-The-Process-of-Data-Analysis-Five-steps-activities-of-the-data-science-process" class="headerlink" title="3. The Process of Data Analysis - Five steps activities of the data science process:"></a><strong>3. The Process of Data Analysis - Five steps activities of the data science process:</strong></h5><ol>
<li><p><strong>Acquire:</strong> includes anything that makes us retrieve data including; finding, accessing, acquiring, and moving data. It includes identification of and authenticated access to all related data. And transportation of data from sources to distributed files systems</p>
</li>
<li><p><strong>Prepare:</strong> </p>
<ul>
<li><strong>Explorer:</strong> The first step in data preparation involves looking at the data to understand its nature, what it means, its quality, and format.</li>
<li><strong>Pre-processing of data:</strong> Pre-processing includes cleaning data, sub-setting or filtering data, creating data, which programs can read and understand, such as modeling raw data into a more defined data model,or packaging it using a specific data format.</li>
</ul>
</li>
<li><p><strong>Analyze:</strong> Select the analytical technique, build models.</p>
</li>
<li><p><strong>Report:</strong> Communicating results includes evaluation of analytical results. Presenting them in a visual way, creating reports that include an assessment of results with respect to success criteria.</p>
</li>
<li><p><strong>Act:</strong> Reporting insights from analysis and determining actions from insights based on the purpose.</p>
</li>
</ol>
<h4 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h4><h5 id="1-Basic-Scalable-Concepts-Scalable-Computing-over-the-Internet"><a href="#1-Basic-Scalable-Concepts-Scalable-Computing-over-the-Internet" class="headerlink" title="1. Basic Scalable Concepts - Scalable Computing over the Internet:"></a><strong>1. Basic Scalable Concepts - Scalable Computing over the Internet:</strong></h5><ul>
<li><strong>Parallel computer:</strong>  a parallel computer is a very large number of single computing nodes with specialized capabilities connected to other networks.</li>
<li><strong>Commodity cluster:</strong> affordable parallel computers with an average number of computing nodes.They are not as powerful as traditional parallel computers and are often built out of less specialized nodes. These types of systems have a higher potential for partial failures. It is this type of distributed computing that pushed for a change towards cost-effective reliable and Fault-tolerant systems for management and analysis of big data</li>
</ul>
<h5 id="2-The-Requirements-For-Big-Data-Programming-Model"><a href="#2-The-Requirements-For-Big-Data-Programming-Model" class="headerlink" title="2. The Requirements For Big Data Programming Model:"></a><strong>2. The Requirements For Big Data Programming Model:</strong></h5><ol>
<li><strong>Support Big Data Operation:</strong><ul>
<li>Split volumes of data</li>
<li>Access data fast</li>
<li>Distribute computations to nodes</li>
</ul>
</li>
<li><strong>Handle Fault tolerance</strong><ul>
<li>Replicate data partition</li>
<li>Recover files when needs</li>
</ul>
</li>
<li><strong>Enable Adding More Racks</strong><ul>
<li>Adding new resources to more or faster data without losing performance (scaling out).</li>
<li>Optimized for specific data types</li>
</ul>
</li>
</ol>
<h5 id="3-Getting-Started-With-Hadoop"><a href="#3-Getting-Started-With-Hadoop" class="headerlink" title="3. Getting Started With Hadoop"></a><strong>3. Getting Started With Hadoop</strong></h5><p><img src="/images/eco.png" alt="Hadoop Ecosystem"></p>
<p><strong>HDFS:</strong> The Hadoop Distributed File System, a storage system for big data.It serves as the foundation for most tools in the <strong>Hadoop ecosystem</strong>.</p>
<p>It provides two capabilities that are essential for managing big data.</p>
<ul>
<li>Scalability to large data sets. </li>
<li>Reliability to cope with hardware failures. By default, HDFS maintains three copies of every block</li>
</ul>
<p><strong>Two key components of HDFS:</strong></p>
<ol>
<li><p><strong>NameNode for Metadata:</strong></p>
<ul>
<li>One namenode per cluster</li>
<li>Coordinator of HDFS cluster</li>
<li>Records the name, location in the directory hierarchy and other metadata</li>
<li>Decides which data nodes to store the contents of the file and remembers this mapping</li>
</ul>
</li>
<li><p><strong>DataNode for block storage</strong></p>
<ul>
<li><strong>Datanode</strong> Runs on each node on the cluster and is responsible for storing the file blocks</li>
<li>listens to commands from the name node for block creation, deletion, and replication. </li>
<li>Replication provides two key capabilities: <ul>
<li>Fault tolerance,  Data locality</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>YARN: The Resource Manager for Hadoop</strong><br>YARN interacts with applications and schedules resources for their use.</p>
<p><strong>Essential Gear in YARN engine:</strong></p>
<ul>
<li><strong>Resource manager:</strong> controls all the resources, and decides who gets what</li>
<li><strong>Application Manager:</strong> It negotiates resource from the Resource Manager and it talks to Node Manager to get its tasks completed</li>
<li><strong>Node Manager:</strong> operates at the machine level and is in charge of a single machine</li>
<li><strong>Container (Machine):</strong> abstract Notions that signifies a resource that is a collection of CPU memory disk network and other resources within the Compute node</li>
</ul>
<p><strong>Mapreduce:</strong> 　　</p>
<p>　　It is a big data programming model that supports all<br>the requirements of big data modeling we mentioned. It can model processing large data, split complications into different parallel tasks and make efficient use of large commodity clusters and distributed file systems.</p>
<ul>
<li>Map: Applys operation to all elements and generates key-values pairs.</li>
<li>Reduce: Summarize operation on elements and construct one output file.</li>
</ul>
<p><strong>MapReduce is bad for:</strong></p>
<ul>
<li>Frequently changing data (slow)</li>
<li>Dependent tasks</li>
<li>Interactive analysis</li>
</ul>
<p><strong>When to reconsider Hadoop:</strong></p>
<ul>
<li><p><strong>Key feature that makes problem Hadoop friendly:</strong></p>
<ul>
<li>Future anticipated data growth</li>
<li>Long term availability of data</li>
<li>Many platforms over single datastore</li>
<li>High Volume, High variety</li>
</ul>
</li>
<li><p><strong>Be careful when:</strong></p>
<ul>
<li>Small dataset</li>
<li>Task Level Parallelism ( Furuethrer analysis for which tool to use in Hadoop ecosystem)</li>
<li>Advanced algorithms (Not all algorithms are scalable in Hadoop, or reducible to one of the programming models supported by YARN</li>
<li>Random Data Access ( you may have to read an entire file just to pick one data entry)</li>
</ul>
</li>
</ul>
<h5 id="4-Cloud-Computing-On-demand-computing-it-enables-us-to-compute-any-time-any-anywhere"><a href="#4-Cloud-Computing-On-demand-computing-it-enables-us-to-compute-any-time-any-anywhere" class="headerlink" title="4. Cloud Computing( On-demand computing: it enables us to compute any time any anywhere)"></a><strong>4. Cloud Computing( On-demand computing: it enables us to compute any time any anywhere)</strong></h5><p><strong>Service Model:</strong></p>
<ul>
<li><p><strong>IaaS:</strong> infrastructure as a service, can be defined as a bare minimum rental service. This is like renting a truck from a company that you can assume has hardware and you do the packing of your furniture, and drive to your new house.</p>
</li>
<li><p><strong>PaaS:</strong> platform as a service,<br>is the model where a user is provided with an entire computing platform.<br>This could include the operating system and programming languages that you need</p>
</li>
<li><p><strong>SaaS:</strong> the software as a service model, is the model<br>in which the cloud service provider takes the responsibilities for the hardware and software environments such as the operating system and the application software</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Online Course</category>
      </categories>
      <tags>
        <tag>Coursera</tag>
        <tag>Big Data</tag>
        <tag>UC San Diego</tag>
      </tags>
  </entry>
  <entry>
    <title>Statistical Learning(Rstudio)1-2</title>
    <url>/2020/07/02/Intro-to-Statistical-Learning/</url>
    <content><![CDATA[<h3 id="Practice-1-Data-Analysis"><a href="#Practice-1-Data-Analysis" class="headerlink" title="Practice 1: Data Analysis"></a>Practice 1: Data Analysis</h3>

	<div class="row">
    <embed src="\me\A1.pdf" width="100%" height="550" type="application/pdf">
	</div>




<a id="more"></a>

<h3 id="Practice-2-logistic-Regression-LDA-QDA-KNN"><a href="#Practice-2-logistic-Regression-LDA-QDA-KNN" class="headerlink" title="Practice 2:logistic Regression,LDA,QDA,KNN"></a>Practice 2:logistic Regression,LDA,QDA,KNN</h3>

	<div class="row">
    <embed src="\me\A2.pdf" width="100%" height="550" type="application/pdf">
	</div>



]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Statistic</tag>
        <tag>Rstudio</tag>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title>Statistical Learning(Rstudio)3-4</title>
    <url>/2020/07/03/tro-to-Statistical-Learning-Rstudio/</url>
    <content><![CDATA[<h3 id="Practice-3-Regression-Tree-Cross-Validation-Mallow-CP-BIC"><a href="#Practice-3-Regression-Tree-Cross-Validation-Mallow-CP-BIC" class="headerlink" title="Practice 3 - Regression Tree, Cross-Validation,Mallow CP, BIC"></a>Practice 3 - Regression Tree, Cross-Validation,Mallow CP, BIC</h3>

	<div class="row">
    <embed src="\me\H3.pdf" width="100%" height="550" type="application/pdf">
	</div>




<a id="more"></a>

<h3 id="Practice-4-SVM-Regressions"><a href="#Practice-4-SVM-Regressions" class="headerlink" title="Practice 4 - SVM, Regressions"></a>Practice 4 - SVM, Regressions</h3>

	<div class="row">
    <embed src="\me\H4.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Statistic</tag>
        <tag>Rstudio</tag>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title>HackerRank 50 Python Practice</title>
    <url>/2020/07/01/HackerRank-Python-Practice/</url>
    <content><![CDATA[<h4 id="1-Write-Function"><a href="#1-Write-Function" class="headerlink" title="1. Write Function"></a>1. Write Function</h4><p>We add a Leap Day on February 29, almost every four years. The leap day is an extra, or intercalary day and we add it to the shortest month of the year, February.<br>In the Gregorian calendar three criteria must be taken into account to identify leap years:</p>
<ul>
<li>The year can be evenly divided by 4, is a leap year, unless:<ul>
<li>The year can be evenly divided by 100, it is NOT a leap year, unless:<ul>
<li>The year is also evenly divisible by 400. Then it is a leap year.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>This means that in the Gregorian calendar, the years 2000 and 2400 are leap years, while 1800, 1900, 2100, 2200, 2300 and 2500 are NOT leap years.</p>
<a id="more"></a>

<p><strong>Task:</strong><br>You are given the year, and you have to write a function to check if the year is leap or not.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_leap</span><span class="params">(year)</span>:</span>   </span><br><span class="line">    <span class="keyword">if</span> year%<span class="number">400</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> year%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">elif</span> year%<span class="number">4</span>==<span class="number">0</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span> </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p><strong>Note:</strong> Pay attention to the order among the if condition statement.</p>
<h4 id="2-List-Comprehensions"><a href="#2-List-Comprehensions" class="headerlink" title="2. List Comprehensions"></a>2. List Comprehensions</h4><p>You are given three integers X,Y  and Z representing the dimensions of a cuboid along with an integer N. You have to print a list of all possible coordinates given by (i,j,k) on a 3D grid where the sum of <strong>i+k+j</strong> is not equal to <strong>N</strong>. Here, **0 $\leq$ i $\leq$ X; 0 $\leq$ j $\leq$ Y; 0 $\leq$ k $\leq$ Z</p>
<p><strong>Sample Input:</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p><strong>Sample Output:</strong></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p><strong>Solution:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    x = int(input())</span><br><span class="line">    y = int(input())</span><br><span class="line">    z = int(input())</span><br><span class="line">    n = int(input())</span><br><span class="line">print([[i,j,k] </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x+<span class="number">1</span>) </span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(y+<span class="number">1</span>) </span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(z+<span class="number">1</span>) </span><br><span class="line"><span class="keyword">if</span> ( ( i + j+k )!=n)])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>HackerRank</category>
      </categories>
      <tags>
        <tag>Code</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Visulization and Analysis</title>
    <url>/2020/06/29/y-First-Data-Visulization/</url>
    <content><![CDATA[<blockquote>
<p>In 2017, I started my first coding practice by using R. Then, I realize that computer language is not rocketscience that I can’t never understand.</p>
</blockquote>
<h3 id="Practice-1"><a href="#Practice-1" class="headerlink" title="Practice 1"></a>Practice 1</h3><h4 id="The-potential-relationship-between-United-States-GDP-and-Some-other-economic-indicators-from-year-1960-to-2015"><a href="#The-potential-relationship-between-United-States-GDP-and-Some-other-economic-indicators-from-year-1960-to-2015" class="headerlink" title="The potential relationship between United States GDP and Some other economic indicators (from year 1960 to 2015)"></a>The potential relationship between United States GDP and Some other economic indicators (from year 1960 to 2015)</h4><p>　The data set pick for first practice is aimed to study the potential relationship between United States GDP and some other economic indicators such as population, exports of goods and service, electric power consumption, unemployment rate and average rate index from year 1960 to 2015. The source of data is from World Bank Open Data ( <em><a href="https://data.worldbank.org" target="_blank" rel="noopener">https://data.worldbank.org</a></em>) and National Average Wage Index (<em><a href="https://www.ssa.gov/oact/cola/AWI.html" target="_blank" rel="noopener">https://www.ssa.gov/oact/cola/AWI.html</a>.</em>)<br>In order to find the relationship, I first need to find out the trend of indicators from 1960 to 2015 and make some logical assumptions in mind.</p>
<a id="more"></a>

<p><strong>Load library needed, read the excel file and rename the columns</strong></p>
<pre><code>library(readxl)
library(ggplot2)
USA_GDP = read_excel(&quot;USA GDP.xlsx&quot;)
names(USA_GDP)
## [1] &quot;Year&quot; &quot;Gdp&quot; &quot;Pop&quot; &quot;Exports&quot; &quot;Electric&quot; &quot;House&quot;
## [7] &quot;Urate&quot; &quot;Wage&quot;</code></pre><p><strong>The Change of indicators from 1960 to 2015</strong></p>
<pre><code>#The change of GDP from 1960 to 2015
ggplot(USA_GDP,aes(Year,Gdp))+geom_line()  </code></pre><p><img src="/images/pasted-16.png" alt="upload successful"></p>
<pre><code>#The change of population from 1960 to 2015
ggplot(USA_GDP,aes(Year,Pop))+geom_line()</code></pre><p><img src="/images/pasted-17.png" alt="upload successful"></p>
<pre><code>#The change of Household final consumption expenditure from 1960 to 2015
ggplot(USA_GDP,aes(Year,House))+geom_line()</code></pre><p><img src="/images/pasted-18.png" alt="upload successful"></p>
<h3 id="Data-Visulization-Project-Tableau"><a href="#Data-Visulization-Project-Tableau" class="headerlink" title="Data Visulization Project( Tableau)"></a>Data Visulization Project( Tableau)</h3><h4 id="Project-Introduction"><a href="#Project-Introduction" class="headerlink" title="Project Introduction"></a>Project Introduction</h4><p>　We may have noticed that recently the United States has brought up several new policies about US tariffs. Especially, when Donald Trump signed a Presidential<br>Memorandum, the United States has announced levies on up to $60 billion in Chinese goods and would impose stiff tariffs on imports of steel and aluminum to all the countries. However, the United States may plan to give exemption to a broad group of allies which include Canada, Mexico. Since China is also a big steel export country,apparently, this policy in some degree will possible have a big effect on Chinese steel and iron industries. We may assume this policy is aimed to against the influx cheap Chinese metal and goods, which may harm American local industry. If the exemption were successfully applied to other countries, China is going to face more severe situation under this circumstance, and the trade battle between U.S and China can possibly turn into a Trade War. This project will analyze basic American import and export of certain taxed product relates to this policy to make logic prediction of the stakes in this competition from my point of views in certain aspects.</p>
<h4 id="Data-Source"><a href="#Data-Source" class="headerlink" title="Data Source"></a>Data Source</h4><p>　All the data can be find in following data website:</p>
<ul>
<li><p>United States Department of Agricultural Economic Research<br><a href="https://www.ers.usda.gov/topics/crops/soybeans-oil-crops/#otherpublications" target="_blank" rel="noopener">https://www.ers.usda.gov/topics/crops/soybeans-oil-crops/#otherpublications</a></p>
</li>
<li><p>The Soybean Processors Association of India<br><a href="http://www.sopa.org/statistics/world-soybean-production/?search_type=search_by_year&amp;years=2017-2018&amp;starting_year_value=&amp;ending_year_value=&amp;submit=Search" target="_blank" rel="noopener">http://www.sopa.org/statistics/world-soybean-production/?search_type=search_by_year&amp;years=2017-2018&amp;starting_year_value=&amp;ending_year_value=&amp;submit=Search</a></p>
</li>
<li><p>World Integrated Trade Solution<br><a href="https://atlas.media.mit.edu/en/profile/country/usa/" target="_blank" rel="noopener">https://atlas.media.mit.edu/en/profile/country/usa/</a></p>
</li>
<li><p>United States Census<br><a href="https://www.census.gov/foreign-trade/balance/index.html" target="_blank" rel="noopener">https://www.census.gov/foreign-trade/balance/index.html</a></p>
</li>
<li><p>UN Comtrade Database<br><a href="https://comtrade.un.org/data" target="_blank" rel="noopener">https://comtrade.un.org/data</a></p>
</li>
</ul>
<h4 id="Research-Questions"><a href="#Research-Questions" class="headerlink" title="Research Questions"></a>Research Questions</h4><ul>
<li>Which countries that United States import and export the most?</li>
<li>Which categories that United States has the most import and export?</li>
<li>If China levy on some American export products, how this policy is going to affect the United States?</li>
</ul>
<h4 id="Final-Data-Visualization"><a href="#Final-Data-Visualization" class="headerlink" title="Final Data Visualization"></a>Final Data Visualization</h4><p>　R and Excel will be used for data cleaning, and Tableau will be considered as my data visualization tools. Since I am going to compare the categories and volume of import and export, bar graphs, heatmaps and pie chart are good methods to express the data. Therefore, Tableau will be my first choice to visualize my data. The data in this project is quite large and complicate, sometimes I have use R to transform my data to fit the format of Tableau. Of course, Excel will also consider as my data editing tool. Visualization types will be:<br>• Bar graph of Overall import and export value from 2000-2017<br>• Line chart of Trade deficit from 2007-2017<br>• Heatmap of Product categories that United States export and import<br>• Pie Charts of Proportion of World Soybean total production</p>
<h4 id="Data-Analysis"><a href="#Data-Analysis" class="headerlink" title="Data Analysis"></a>Data Analysis</h4><h5 id="U-S-2000-2017-Overall-Import-and-Export-goods-and-service"><a href="#U-S-2000-2017-Overall-Import-and-Export-goods-and-service" class="headerlink" title="U.S 2000 - 2017 Overall Import and Export (goods and service)"></a>U.S 2000 - 2017 Overall Import and Export (goods and service)</h5><p><img src="/images/pasted-23.png" alt="upload successful"><br><img src="/images/pasted-24.png" alt="upload successful"></p>
<ul>
<li>According to the bar graph of U.S overall Import and Export. From 2000 – 2017, the export and import appear rising tendency. But the trade deficit reach the highest in 2006 and reduce to 384 billions dollars then kept fluctuating around 500 billions dollars. In order to reduce the trade deficit, Trump brought up a series of trading policy on export and import.</li>
</ul>
<h5 id="March-2018-The-Value-of-U-S-Export-and-Import-To-Top-15-Countries-goods-only"><a href="#March-2018-The-Value-of-U-S-Export-and-Import-To-Top-15-Countries-goods-only" class="headerlink" title="March 2018, The Value of U.S Export and Import  To Top 15 Countries. (goods only)"></a>March 2018, The Value of U.S Export and Import  To Top 15 Countries. (goods only)</h5><p><img src="/images/pasted-27.png" alt="upload successful"></p>
<p><img src="/.io//images%5Cpasted-28.png" alt="upload successful"></p>
<ul>
<li>By comparing two bar graphs, we can easily see that Canada, Mexico, China are top three countries in the value of American import and export. Canada is the biggest American exportation country. And China is the biggest American importation country. Since America relies on China more than China rely on America, we may logically assume once China start its retaliatory tariffs on US exports, the United State may have more loss than Chinese.</li>
</ul>
<h5 id="Products-that-United-States-Imports-from-China-in-2017"><a href="#Products-that-United-States-Imports-from-China-in-2017" class="headerlink" title="Products that United States Imports from China in 2017"></a>Products that United States Imports from China in 2017</h5><p><img src="/images/pasted-35.png" alt="upload successful"></p>
<ul>
<li>From the heatmap, we can find that the major categories that United States import are cell phones and other household goods, computer and computer accessories, telecommunication equipment and so on… These are major areas on which Donald Trump wants to apply tariffs, in order to slow down or restrict the rising trend of China taking dominant in these areas(high-tech products), and try to take America back into the leading position in these areas.</li>
</ul>
<h5 id="Products-that-United-States-Exports-to-China-in-2017"><a href="#Products-that-United-States-Exports-to-China-in-2017" class="headerlink" title="Products that United States Exports to China in 2017"></a>Products that United States Exports to China in 2017</h5><p><img src="/images/pasted-36.png" alt="upload successful"></p>
<ul>
<li>From the heatmap, we can find that the major categories that United States export to China are civilian aircraft, engines, soybeans, Passenger cars, semi-products. Those categories will be the threatens to United States and have a big chance becoming the target of China as revenge. As far as we know, Chinese government has announced that it will impose 15% tariffs on 128 American made products include fruit, nuts and wine and up to 25% on pork as initially fight back. Furthermore, Chinese government will probably keep imposing tariffs on more products made by United States depend on how the battle goes. </li>
</ul>
<h5 id="Threaten-Tariffs-on-Soybeans"><a href="#Threaten-Tariffs-on-Soybeans" class="headerlink" title="Threaten - Tariffs on Soybeans"></a>Threaten - Tariffs on Soybeans</h5><p><img src="/images/pasted-38.png" alt="upload successful"></p>
<ul>
<li>Obviously, the United State and Brazil are two major soybeans producers as well as exporters in this world. If China imposes tariffs on soybean, which means it gives up one of the largest soybean import resource in the world, I believe that China won’t insist for a long time because there is no other supplier except U.S can provide such tremendous amount of soybeans.  </li>
</ul>
<p><img src="/images/pasted-43.png" alt="upload successful"></p>
<p><img src="/images/pasted-44.png" alt="upload successful"></p>
<ul>
<li><p>By comparing two graphs, it is easy to see the export pattern of two countries.  Most of soybeans in brazil were exported to China in the period from March to September. In rest of the year , China has imported soybeans from the United States, which is the reason I consider that China will keep suffering from the loss until it find a way to solve  domestic demand of soybeans. In a short period, Brazil and Argentina would be the best choice and support for China to pass through the “harsh time”. However, It is hard for China to find suppliers that can really fill China’s soybeans needs for a long term. The structure of global supply chain is not that easy to change.</p>
</li>
<li><p>Moreover, We may consider another situation: Is it possible for Brazil to provide enough soybean to fulfill the demand of China? The answer is No. Maybe there are several ways that can mitigate the reliance on American soybeans such as importing more soybeans from Brazil and Argentina; however, there is no way that China can stay away from the loss. </p>
</li>
</ul>
<h5 id="Threaten-Tariffs-on-Pork-and-Meat-Products"><a href="#Threaten-Tariffs-on-Pork-and-Meat-Products" class="headerlink" title="Threaten - Tariffs on Pork and Meat Products"></a>Threaten - Tariffs on Pork and Meat Products</h5><p><img src="/images/pasted-41.png" alt="upload successful"></p>
<ul>
<li>Broiler and pork are the main meat export product of United States, which take up42.5% and  32.35% of total meat exportation respectively. If China wants to increase the tax on meat product, the United State would have big effect on domestic farmers only if China is the big pork and broiler importer of the United States.</li>
</ul>
<h4 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h4><ul>
<li><p>The effect of Tariffs between two large countries has always been a complicate issue. The change of tariffs in a large will not only affect domestic supply and demand or domestic price, but also will have big global effect. Taking a simple example in this case, the policy that China imposes tariffs on soybeans will definitely benefit to Brazil, Argentina and all other countries who have ability to provide soybeans. </p>
</li>
<li><p>This project can expend to many areas. Base on the information I concluded, we just need to gather a little bit more of data on globe soybean price to calculate the effect of tariffs on the volume change of Import and export as result. </p>
</li>
<li><p>We can furthermore investigate whether the tariffs on Auto has really harmed the interest of those Giant Auto companies. Because Tariffs is such a complicate topic, we can apply many economic concepts and do the research</p>
</li>
</ul>
<h4 id="Conclusion-and-Prediction"><a href="#Conclusion-and-Prediction" class="headerlink" title="Conclusion and Prediction"></a>Conclusion and Prediction</h4><p>　All the activities between China and the United State are just the “normal”friction in the international trade. I don’t believe that the tension and friction between　these two superpower countries so far will escalate into the real Trade War. And I don’t think they will even allow this happen. We know China and America play vital roles in the global economy, the chain reaction of trade war in global and national economy is going to be destructive; therefore, there is no way for both China and the United State to be involve in a long-term trade battle. China won’t, neither Donald Trump. Overall, people are responsible for countries’ international trade policies. Even though statistics says the effect of certain policy is minor, the loss is on people and people who has business involved. It is simple logic for everyone to understand that collaboration between two countries can lead to win-win situation. Negotiation is the best way to reduce the friction and solve problems, which is what China and America are doing right now. If two countries reach a common agreement, the trade battle will eventually end.</p>
]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Tableau</tag>
        <tag>Data analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>Web-Mining Full Project Report</title>
    <url>/2020/06/26/web-Mining-Full-Project-Report/</url>
    <content><![CDATA[<blockquote>
<p>In retrospect my first NLP project,it was a really great learning experience that got me through the process of nature language processing, and piqued my interest to the Data Science.</p>
</blockquote>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>　In the past one year, PUBG became one of the most popular games in the word and at the same time it also received thousands of negative comments and thus I planned to start a project to help those developers and operators extract valuable information from those critics. The purpose of my project is to get suggestions from those critics, and then I plan to assign labels for players manually to classify those clear-minded supporters and critics and then train some classification models to discover more critics with firm stand.Finally, I will propose some future improvements for this project which I could have done better and also as reference for my next project.</p>
<a id="more"></a>

<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>　PUBG, one of the most popular shooting games on steam, was published by PUBG Corporation, a subsidiary of South Korean video game company Bluehole. This game was released officially in September 2018. The game is one of the best-selling of all time, with over fifty million sold across all platforms by June 2018. In addition, the Windows version holds a peak concurrent player count of over three million on Steam, which is an all-time high on the platform. PUBG received thousands of reviews from players, who found that there are still many bugs and problems existed in the game.Thus, I decide to research on discovering useful information from those reviews.</p>
<h3 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h3><p>　In this project, I am trying to detect some useful information from the reviews and have a deep understanding of the game by analysis crawled from the Steam reviews community. After this project, I hope my analysis will help game developers identify the problems that most players concerned about.</p>
<h3 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h3><p>　In this project, the data source comes from Steam.I use <strong>Selenium</strong> to scrape some reviews page.I use CSS selectors to get all the attributes including user comment date, user reviews, whether users recommend or not, how many hours spent on the game, and how many games users hold in the account.</p>
<h3 id="Data-Description"><a href="#Data-Description" class="headerlink" title="Data Description"></a>Data Description</h3><p>　The data size is 2070. The data contains three independent variables including spent hours, hold products and user reviews, and one dependent variable which is “recommended or not”. Spent hours refers to how many hours that players have spent on the game. Hold products refers to how many games user hold in their accounts. User reviews refer to what players comment on the website. Recommend or not refers to whether the player recommends this game or not. The sample data presented below:<br><img src="/images/pasted-1.png" alt="upload successful"></p>
<p>　On the platform, each user can easily write down their feedbacks toward the game and they will assign a label of whether they want to recommend or not-recommend this game to other players. Steam will also list the information of users such as hours of playing, the Steam products in the user’s account, and other players’ attitudes toward this comment. This review system will give the game companies or developers an overall insight into how this running on this platform by collecting all the data from users’ reviews. The user interface shows below:<br> <img src="/images/pasted-2.png" alt="upload successful"></p>
<h3 id="Data-cleaning"><a href="#Data-cleaning" class="headerlink" title="Data cleaning"></a>Data cleaning</h3><p>　The data contains some symbols and stop words which cause problems for me to find meaningful information from the user reviews and thus I remove these noise and transfer all characters into lower cases and split the reviews into words.</p>
<h3 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a>Exploratory Data Analysis</h3><p>　Next, I did some simple exploratory data analysis to find the correlation among these variables. Firstly, I made a bar chart to present the relation between spent hours  and recommendations<br> <img src="/images/pasted-3.png" alt="upload successful"><br>As the bar chart presents above, I find that the people who have spent more than 1900 hours on PUBG tend to recommend this game. In other words, this game is very popular with old players, and thus the reviews made by this part of players are more meaningful for the operator.<br>And then, I made a scatter plot to explore the relation between spent hours and hold products.</p>
<p><img src="/images/pasted-6.png" alt="upload successful"></p>
<h3 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h3><p>　In this part, I use both supervised learning algorithms and unsupervised learning algorithms to make models.</p>
<h4 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h4><h5 id="VADER"><a href="#VADER" class="headerlink" title="VADER"></a>VADER</h5><p>　<strong>VADER</strong> analyzes a piece of text to see if any of the words in the text is present in the lexicon. Sentiment metrics are derived from the ratings of such words positive, neutral, and negative, represent the proportion of the text that falls into those categories.The final metric, the compound score, is the sum of all the lexicon ratings which have been standardized to range between -1 and 1 based on some heuristics.In my project, I want to see users’ sentiment for each of the reviews by applying VADER analysis, and group those negative sentiment reviews for company inspect.</p>
<p><img src="/images/pasted-7.png" alt="upload successful"></p>
<p>　From the result shown above, reviews have been classified into three categories: positive sentiment, neutral sentiment, and negative sentiment.In positive sentiment,77.45% of reviews have been correctly　classified;In neutral sentiment,75.33 are recommended reviews and 24.66% are not recommended reviews; In negative sentiment, 26.38% negative reviews　have been correctly classified.I visualize the model result.The model has bad performance on negative sentiment. And then I try to find the reasons.After I dig into the reviews, I find out the reviews sometimes full of sarcasm, the computer is not capable of recognizing the emotion behind the sentences. For example: “This game used to be good, but now it is just game for cheaters.” Then I got a conclusion that the labels in the review system didn’t truly reflect the attitudes of players and the game developer can also possibly mislead by the data of labels.</p>
<h5 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h5><p>　Due to the massive information of reviews, it’s difficult for operators to analyze the reviews one by one. So, I use LDA (Latent Dirichlet allocation) to generate topics for the document. This model reduces the dimensionalities of words and thus it worked more efficiently compared with bag-of-words model and got rid of overfitting. I visualized three top topics as below:</p>
<p><img src="/images/pasted-9.png" alt="upload successful"><br>　Through these pictures, I found more negative words than positive words. These words state that the server of the game is bad and mountains of cheaters in this game and this game didn’t make any improvement over time. This result may confuse us why there are more negative words about this game, and in the EDA stage I found more people recommended this game.In the VADER model, I have found that 75.33% of people who have neutral attitudes have been classified as supporters. And by reading some comments, I found that these people’s comments contain a lot of negative words and that’s the reason why I found so many negative words here. Through the LDA and VADER model, I got a conclusion that I cannot simply classify people by their comments’ labels, and thus, I planned to select those clear-minded comments to train the classification model to find more players with a firm stand.</p>
<h4 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h4><p>　The classification models are applied to detect positive and negative users. Since there are a great many reviews are not labeled in other sources. It is necessary to classify a large number of reviews and get directions for later improvement, especially from the negative sentiments.</p>
<h5 id="Multinomial-Naive-Bayes"><a href="#Multinomial-Naive-Bayes" class="headerlink" title="Multinomial Naïve Bayes"></a>Multinomial Naïve Bayes</h5><p>　<strong>Multinomial Naïve Bayes</strong> is suitable for classification with word counts for text classification, such as TF-IDF weight.<br>As the result shows above, this model has a good performance. The precision rate is 0.84 and the recall rate is 0.82.<br><img src="/images/pasted-11.png" alt="upload successful"></p>
<h5 id="Support-Vector-Classification"><a href="#Support-Vector-Classification" class="headerlink" title="Support Vector Classification"></a>Support Vector Classification</h5><p>　<strong>Support vector machine</strong> constructs a hyper-plane or set of hyper-planes in a high or infinite-dimensional space, which can be used for classification. As the pictures are shown above, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class. It has 0.85 of precision rate and 0.83 of recall rate.</p>
<p><img src="/images/pasted-12.png" alt="upload successful"></p>
<h5 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a><strong>Logistic Regression</strong></h5><p>　<strong>Logistic Regression</strong> model the probabilities of a recommendation as a linear function of documents term matrix and classify reviews into two categories based on TF-IDF weights of bag-of-words.<br>As the pictures are shown above, it has 84% precision rate and 80% recall rate. In summary, MNB, SVM, and Logistic Regression have good performance.</p>
<p><img src="/images/pasted-13.png" alt="upload successful"></p>
<h5 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a><strong>CNN</strong></h5><p>　<strong>CNN</strong> model is introduced since it has advantages in imbalanced data classification. The doc2vec and word2vec are used to train a large number of unlabeled reviews to generate a fixed weights word matrix to map the word in the embedding phase. The doc2vec is different in predicting the word by concatenating the paragraph vector D (shared within the paragraph). The words vector trained by Doc2Vec has a better performance when checking the similarities of the words.<br>The figure describes the top 5 most similar word with ‘play’:</p>
<p><img src="/images/pasted-14.png" alt="upload successful"><br>The CNN model is using three sizes of filters (bigram, trigram, and quadrigram), each size has 64 filters in the convolutional layer.</p>
<p><img src="/.io//images%5Cpasted-15.png" alt="upload successful"><br>In summary, the CNN model that using a pre-trained matrix has a better performance as the picture shown above.</p>
<h3 id="Word-Interpretation"><a href="#Word-Interpretation" class="headerlink" title="Word Interpretation"></a>Word Interpretation</h3><p>　To get some insights from negative sentiment reviews, I use TFIDF weights and Word2Vec to dig out content from reviews.<br>As the picture shows, I picked the top ten words which have the highest frequency. Some words such as bad, time, money are keywords that might point to the potential problem.<br>Next step, I use Word2Vec to find the most similar words with these top 10 words, trying to find correlations between targeted words.<br>As a result, the problems found are as follows:</p>
<ol>
<li>A lot of game issues</li>
<li>Serious time delay for server lags</li>
<li>Devs(Developer) fix the bug</li>
<li>Vehicle issue in the game</li>
<li>Waste money</li>
<li>A lot of cheaters</li>
</ol>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>　In unsupervised learning, I found that the label in the dataset cannot accurately describe the attitude of players, but the purpose of my project is to excavate valuable information from critics and thus it’s very important to train a model to find those critics with frim stand. And then I trained several classification models including Multinomial Naïve Bayes, SVM, Logistic Regression, and CNN, and found that CNN has the best performance among these models. Finally, I explain the words meaning after classification and then give the suggestion based on what I got from the result and raise the suggestion to the company. Wipe out cheaters from the game, it hugely impacts the user’s game experience. Give more support to users when they meet with game issues. Developers need to put more effort to optimize game and fix those existed bugs. Cautiously deal with the micro-transaction, make it reasonable to users.</p>
<h3 id="Future-Improvement"><a href="#Future-Improvement" class="headerlink" title="Future Improvement"></a>Future Improvement</h3><p>　There are still some improvements that I can make in the future. For example, I can try the Steam Game Platform API,and I can improve VADER by updating the word list. Since VADER is defined as Sentiment metrics techniques, I expect to improve model performance by updating the word list and re-training the model.</p>
]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
        <tag>Web Scraping</tag>
      </tags>
  </entry>
  <entry>
    <title>HackerRank 30-days Python</title>
    <url>/2020/06/04/My-second-post-to-test/</url>
    <content><![CDATA[<blockquote>
<p>This post only records the problems I made mistakes.</p>
</blockquote>
<h4 id="Intro-to-Conditional-Statements"><a href="#Intro-to-Conditional-Statements" class="headerlink" title="Intro to Conditional Statements"></a>Intro to Conditional Statements</h4><p><strong>Task:</strong> Given an integer,n, perform the following conditional actions:</p>
<ul>
<li>If  is odd, print Weird</li>
<li>If  is even and in the inclusive range of  to , print Not Weird</li>
<li>If  is even and in the inclusive range of  to , print Weird</li>
<li>If  is even and greater than , print Not Weird</li>
</ul>
<p>Complete the stub code provided in your editor to print whether or not is weird.</p>
<a id="more"></a>

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">   n = int(input().strip())</span><br><span class="line">   <span class="keyword">if</span> n%<span class="attribute">2</span>==0 <span class="keyword">and</span> (n <span class="keyword">in</span> range (2,6) <span class="keyword">or</span> n&gt;20):</span><br><span class="line">   		<span class="builtin-name">print</span>(<span class="string">'Not Weird'</span>)</span><br><span class="line">   <span class="keyword">if</span> n%<span class="attribute">2</span>==1 <span class="keyword">or</span> (n%<span class="attribute">2</span>==0 <span class="keyword">and</span> (n <span class="keyword">in</span> range(6,21))):</span><br><span class="line">      <span class="builtin-name">print</span>(<span class="string">'Weird'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Note:</strong> When think about the condition, we can simply catergorize the condition and try whether we can put them in one statement, which will make the codes cleaner.</p>
<h4 id="Class-vs-Instance"><a href="#Class-vs-Instance" class="headerlink" title="Class vs. Instance"></a>Class vs. Instance</h4><p><strong>Task:</strong> Write a Person class with an instance variable,<strong><em>age</em> <em>*,and a constructor that takes an integer,</em></strong>initialAge<strong>* , as a parameter. The constructor must assign *</strong>initialAge<strong>* to <em>age</em> after confirming the argument passed as *</strong>initialAge<strong>* is not negative; if a negative argument is passed as *</strong>initialAge<strong>*, the constructor should set *</strong>age*** to 0 and print Age is not valid, setting age to 0.. In addition, you must write the following instance methods:</p>
<ol>
<li>yearPasses() should increase the instance variable by 1.</li>
<li>amIOld() should perform the following conditional actions:<ul>
<li>If <strong>age &lt; 13</strong> , print You are young..</li>
<li>If <strong>age $\geq$ 13</strong> and <strong>age &lt; 18</strong>, print You are a teenager..</li>
<li>Otherwise, print You are old..</li>
</ul>
</li>
</ol>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">class Person:</span><br><span class="line">    age = 0</span><br><span class="line">    def __init__(self,initialAge):</span><br><span class="line">        # <span class="builtin-name">Add</span> some more code <span class="keyword">to</span> <span class="builtin-name">run</span> some checks on initialAge</span><br><span class="line">        <span class="keyword">if</span> initialAge &lt; 0:</span><br><span class="line">            <span class="builtin-name">print</span> (<span class="string">"Age is not valid, setting age to 0."</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.age = initialAge</span><br><span class="line">    def amIOld(self):</span><br><span class="line">        # <span class="keyword">Do</span> some computations <span class="keyword">in</span> here <span class="keyword">and</span> <span class="builtin-name">print</span> out the correct statement <span class="keyword">to</span> the console</span><br><span class="line">        <span class="keyword">if</span> self.age &lt; 13:</span><br><span class="line">            <span class="builtin-name">print</span> (<span class="string">"You are young."</span>)</span><br><span class="line">        elif self.age &gt;= 13 <span class="keyword">and</span> self.age &lt; 18:</span><br><span class="line">            <span class="builtin-name">print</span> (<span class="string">"You are a teenager."</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="builtin-name">print</span> (<span class="string">"You are old."</span>)</span><br><span class="line">    def yearPasses(self):</span><br><span class="line">        # Increment the age of the person <span class="keyword">in</span> here</span><br><span class="line">        self.age += 1</span><br></pre></td></tr></table></figure>
<h4 id="Loops"><a href="#Loops" class="headerlink" title="Loops"></a>Loops</h4><p><strong>Task</strong>: Given an integer, , print its first  multiples. Each multiple  (where ) should be printed on a new line in the form: n x i = result.</p>
<p><strong>input Format:</strong> A single integer,<strong>n</strong></p>
<p><strong>Constraints:</strong> 2 $\leq$ n $\leq$ 20 </p>
<p><strong>Output Format:</strong></p>
<p>Print <strong>10</strong> lines of output; each line <strong>i</strong> (where 1 \leq i \leq 10) contains the <strong>result</strong> of <strong>n x i</strong> in the form:<br><strong>n x i = result</strong></p>
<p>Sample Input: 2</p>
<p>Sample Output:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">2</span> x <span class="number">1</span> = <span class="number">2</span></span><br><span class="line"><span class="number">2</span> x <span class="number">2</span> = <span class="number">4</span></span><br><span class="line"><span class="number">2</span> x <span class="number">3</span> = <span class="number">6</span></span><br><span class="line"><span class="number">2</span> x <span class="number">4</span> = <span class="number">8</span></span><br><span class="line"><span class="number">2</span> x <span class="number">5</span> = <span class="number">10</span></span><br><span class="line"><span class="number">2</span> x <span class="number">6</span> = <span class="number">12</span></span><br><span class="line"><span class="number">2</span> x <span class="number">7</span> = <span class="number">14</span></span><br><span class="line"><span class="number">2</span> x <span class="number">8</span> = <span class="number">16</span></span><br><span class="line"><span class="number">2</span> x <span class="number">9</span> = <span class="number">18</span></span><br><span class="line"><span class="number">2</span> x <span class="number">10</span> = <span class="number">20</span></span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">   n = int(input())</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range (1,11):</span><br><span class="line">       <span class="attribute">result</span>=n*i</span><br><span class="line">       <span class="builtin-name">print</span>(<span class="string">'%s x %s = %s'</span>%(n,i,result))</span><br></pre></td></tr></table></figure>
<p><strong>Note</strong>: The format string expression: <strong>print(‘%s x %s = %s’%(n,i,result))</strong>. Use tuple <strong>(n,i,result)</strong> to pass value into the formatted experession.</p>
]]></content>
      <categories>
        <category>HackerRank</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>HackerRank</tag>
        <tag>30 Days Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Quantium Virtual Internship </title>
    <url>/2020/07/13/uantium-Virtual-Internship/</url>
    <content><![CDATA[<blockquote>
<p>We bring expertise in two core domains to make the most of opportunities in the ‘big data world’– Quantium</p>
</blockquote>
<h3 id="Task-1-–-Data-preparation-and-customer-analytics"><a href="#Task-1-–-Data-preparation-and-customer-analytics" class="headerlink" title="Task 1 – Data preparation and customer analytics"></a>Task 1 – Data preparation and customer analytics</h3><h4 id="Background"><a href="#Background" class="headerlink" title="Background:"></a>Background:</h4><p>You are part of Quantium’s retail analytics team and have been approached by your client, the Category Manager for Chips, who wants to better understand the types of customers who purchase Chips and their purchasing behaviour within the region.<br>The insights from your analysis will feed into the supermarket’s strategic plan for the chip category in the next half year.</p>
<h4 id="End-Goal"><a href="#End-Goal" class="headerlink" title="End Goal:"></a>End Goal:</h4><p>To form a strategy based on the findings to provide a clear recommendation to Category Manager so make sure your insights can have a commercial application.</p>
<h4 id="Early-Exploratory-analysis"><a href="#Early-Exploratory-analysis" class="headerlink" title="Early Exploratory analysis"></a>Early Exploratory analysis</h4><ul>
<li>Examine transaction data </li>
<li>Examine customer data</li>
<li>Data analysis and customer segments</li>
<li>Deep dive into customer segments</li>
<li>Derive extra features (pack size and brand name)and define metrics of interest to enable you to draw insights on who spends on chips and what drives spends for each customer segment</li>
</ul>
<h4 id="Data-Cleaning-and-Fixing"><a href="#Data-Cleaning-and-Fixing" class="headerlink" title="Data Cleaning and Fixing"></a>Data Cleaning and Fixing</h4><p><strong>Step 1: Basic Checking and Editing by Excel</strong></p>
<ul>
<li><p>View the data in Excel and apply <strong>Filter</strong> to check the missing values in each columns – <strong>No missing values in the dataset</strong>.</p>
</li>
<li><p>Use <strong>short date format</strong> to change date columns to normal date expression. However, there are some outliers in the dataset,the card number 226000 only have two transactions in the whole dataset, so I decide to remove them.</p>
</li>
<li><p>By observing the product names using Filter, I find some similar names such as WW - Woolworths,CCs - Cheezels Cheese,Infzns - Infzions, NCC-Natural Chip - Natural Chip Company, RRD - Red Rock Deli, Smith - Smiths, Snbts- Sunbites, GrnWves - Grain Waves, which I decide to fix using Python for simplicity.</p>
</li>
<li><p>Load the data in Python for futher checking and operation.</p>
</li>
</ul>
<p><strong>Step 2: Data Manipulation by Python</strong></p>
<ol>
<li>Loda the Transaction Data as <strong>Tdata</strong>, Load the purchase data as <strong>Pdata</strong> and try to merge two data as one,since Transaction Data and Purchase data share the same membership card column</li>
</ol>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">Tdata=pd.read_excel(<span class="string">r'C:\Users\Administrator\Desktop\QVI_transaction_data.xlsx'</span>)</span><br><span class="line">Pdata=pd.read_csv(<span class="string">r'C:\Users\Administrator\Desktop\QVI_purchase_behaviour.csv'</span>)</span><br><span class="line">data=Tdata.merge(Pdata,on= <span class="string">'LYLTY_CARD_NBR'</span>)</span><br><span class="line">data.info() <span class="comment"># By checking the info of data, we didn't find any strange in the dataset.</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>Because we are focusing on chip category , we only want to keep the chip products. So let’s create a new dataframe for chip prodcut.</li>
</ol>
<figure class="highlight php"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use str.contains to find the product name that contain the 'chip'. And Create a new dataframe for chips only.</span></span><br><span class="line">chipdata=data[data[<span class="string">'PROD_NAME'</span>].str.contains(<span class="string">'chip'</span>,<span class="keyword">case</span>=<span class="keyword">False</span>)] <span class="comment"># Case sensitive = False</span></span><br><span class="line">chipdata.reset_index(drop=<span class="keyword">True</span>,inplace=<span class="keyword">True</span>) <span class="comment"># If you don't want it saved as a column then drop = True, inplace= True(reassign the index)</span></span><br><span class="line"><span class="keyword">print</span>(chipdata.head())</span><br><span class="line">chipdata.duplicated().any()<span class="comment"># check duplicated data</span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li>Fix the product names to make product name consistent</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Clean all the special character in the product name</span></span><br><span class="line">chipdata[<span class="string">'PROD_NAME'</span>] = chipdata[<span class="string">'PROD_NAME'</span>].str.replace(<span class="string">'\W'</span>, <span class="string">''</span>)</span><br><span class="line"><span class="comment"># Regular expression is the key point to understand the formula</span></span><br><span class="line"><span class="comment"># Replace all the name to standard format</span></span><br><span class="line">chipdata[<span class="string">'PROD_NAME'</span>]=chipdata[<span class="string">'PROD_NAME'</span>].replace(&#123;<span class="string">'WW'</span>: <span class="string">'Woolworths'</span>,<span class="string">'CCs'</span>:<span class="string">'Cheezels Cheese'</span>,<span class="string">'Infzns'</span>:<span class="string">'Infzions'</span>,<span class="string">'Natural Chip Co'</span>: <span class="string">'Natural Chip Company'</span>,<span class="string">'NCC'</span>:<span class="string">'Natural Chip Company'</span>,<span class="string">'RRD'</span> :<span class="string">'Red Rock Deli'</span>, <span class="string">'Smith'</span>:<span class="string">'Smiths'</span>,<span class="string">'Snbts'</span>:<span class="string">'Sunbites'</span>,<span class="string">'GrnWves'</span>:<span class="string">'Grain Waves'</span>&#125;,regex=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># check to see what product name we have </span></span><br><span class="line">chipdata[<span class="string">'PROD_NAME'</span>].unique()</span><br><span class="line"><span class="comment"># From the result we can see that we have some problem on Smithss</span></span><br><span class="line">chipdata[<span class="string">'PROD_NAME'</span>]=chipdata[<span class="string">'PROD_NAME'</span>].replace(&#123;<span class="string">'Smithsss'</span>:<span class="string">'Smiths'</span>&#125;,regex=<span class="literal">True</span>)</span><br><span class="line">chipdata[<span class="string">'PROD_NAME'</span>].unique()<span class="comment"># now it appears normall</span></span><br></pre></td></tr></table></figure>
<p>Now the data is good for customer analysis.For simplicity, we can load the data into Tableau for visualization analysis</p>
<h3 id="Customer-Analysis"><a href="#Customer-Analysis" class="headerlink" title="Customer Analysis"></a>Customer Analysis</h3><p>**Lets figure out several questions that may help us understand the customer behavior:</p>
]]></content>
  </entry>
  <entry>
    <title>KPMG Vitrual Internship Notes</title>
    <url>/2020/06/02/posttry-to-keep-my-website-runing/</url>
    <content><![CDATA[<h3 id="Task-1-Identifying-the-data-quality-issues-and-how-this-may-impact-our-analysis-going-forward"><a href="#Task-1-Identifying-the-data-quality-issues-and-how-this-may-impact-our-analysis-going-forward" class="headerlink" title="Task 1: Identifying the data quality issues and how this may impact our analysis going forward?"></a>Task 1: Identifying the data quality issues and how this may impact our analysis going forward?</h3><p>Since the company didn’t define its own business definition for data quality evaluation, I will follow the general process.</p>
<p>To evaluate the quality of data, we can follow the Six Standard Data Quality Dimension: </p>
<a id="more"></a>

<ul>
<li><strong>Accuracy</strong>: The degree to which the data correctly describe the ‘real-world’ objects. <strong>Example:</strong> <em>if a man is 30 years old, but the data is 35 years old.</em></li>
<li><strong>Completeness</strong>: Whether the data fulfill the expectation of business comprehensiveness; in other words, whether you can extract the information you want from the data.</li>
<li><strong>Uniqueness</strong>: Make sure there is no duplicate record in the dataset. <strong>Example</strong>: <em>There are 300 students in total, but there are 350 records</em></li>
<li><strong>Timeliness</strong>：Make sure that the data is recorded at the time when it occurred. No delay.</li>
<li><strong>Validity</strong>：Data are valid if it conforms to the syntax (format, type, range) of its definition.**</li>
<li><strong>Consistency</strong>：The data should be the same as input in other columns.<strong>Example</strong>: <em>If a student name:’Peter Pan’ is in the class name-list, then ‘Peter Pan’in the school name-list should be the same as class name-list.</em> </li>
</ul>
<p><a href="https://www.whitepapers.em360tech.com/wp-content/files_mf/1407250286DAMAUKDQDimensionsWhitePaperR37.pdf" target="_blank" rel="noopener">Reference: Defining Data Quality Dimensions</a></p>
<h4 id="Task-1-Problems-Procedures-and-Notes-Excel-and-Python"><a href="#Task-1-Problems-Procedures-and-Notes-Excel-and-Python" class="headerlink" title="Task 1 Problems,Procedures and Notes (Excel and Python)"></a>Task 1 Problems,Procedures and Notes (Excel and Python)</h4><h5 id="load-the-file-in-Python-and-check-the-sheets-in-file"><a href="#load-the-file-in-Python-and-check-the-sheets-in-file" class="headerlink" title="load the file in Python and check the sheets in file"></a>load the file in Python and check the sheets in file</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># load the necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line"><span class="keyword">import</span> xlwt</span><br><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> date</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the file and check the sheet names</span></span><br><span class="line">data=pd.ExcelFile(<span class="string">'C:/Users/Administrator/Desktop/KPMG_VI_New_raw_data_update_final.xlsx'</span>)</span><br><span class="line">names=data.sheet_names</span><br><span class="line">names</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the null values exisit in the sheets to deteremine whether will affect our analysis.</span></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">	null=data.parse(name).isnull().sum()</span><br><span class="line">   print(null)</span><br></pre></td></tr></table></figure>
<ol>
<li>After I observe through the summary of null values, I found the missing values have little impact on our dataset because we want to foucs on the customer that have the relativly big purchase power, we only need to make sure that the customer ID and list price are not null.</li>
<li>And I also find four unnamed columns. By checking the function, lets define those columns. Unnamed 1: Random number between 40-110 Unnamed 2: If customer has car, then use random number times 1.25, otherwise times 1. Unnamed 3: If the property valuation bigger than 8 or past 3 years bike related purchases bigger than 80, then use number from Unnamed 2 times 1.25, otherwise times 1. Unnamed 4: If customer was considered as mass customer then use number from Unnamed 3 times 0.85, otherwise times 1.</li>
<li>Not Sure what those indicator use for, I decide to leave blank.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Now, lets merge the other sheets with Transaction data on Customer ID</span></span><br><span class="line"><span class="comment"># Note: pd.merge can only merge two dataframe.</span></span><br><span class="line">Tdata=data.parse(<span class="string">'Transactions'</span>) <span class="comment"># Read Transaction data</span></span><br><span class="line">Cdata=data.parse(<span class="string">'CustomerDemographic'</span>)</span><br><span class="line">Adata=data.parse(<span class="string">'CustomerAddress'</span>)</span><br><span class="line">data2=Tdata.merge(Cdata, on= <span class="string">'customer_id'</span>)</span><br><span class="line">data3=data2.merge(Adata, on= <span class="string">'customer_id'</span>)</span><br><span class="line"><span class="comment">## Delete all the rows tht contain the missing values and check the result</span></span><br><span class="line">data4=data3.dropna(how=<span class="string">'any'</span>)</span><br><span class="line">data4.isnull().sum()</span><br></pre></td></tr></table></figure>
<h4 id="Task-2-We-want-to-evaluate-the-customer-value-to-determine-our-target-customer"><a href="#Task-2-We-want-to-evaluate-the-customer-value-to-determine-our-target-customer" class="headerlink" title="Task 2: We want to evaluate the customer value to determine our target customer"></a>Task 2: We want to evaluate the customer value to determine our target customer</h4><p><strong>Right here, we can apply the classic customer evaluation model – RFM model</strong></p>
<ul>
<li>I decide to add 12-31-2017 as comparision date for calculating the number of recency.</li>
<li>Apply percentile to determine R score</li>
<li>Adding the number of R score and M score as well as RFM score</li>
<li>Create a RFM dataframe</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Calculate the number of days </span></span><br><span class="line">data4[<span class="string">'daydiff'</span>]=dt.datetime(<span class="number">2017</span>,<span class="number">12</span>,<span class="number">30</span>)- data4[<span class="string">'transaction_date'</span>] </span><br><span class="line"><span class="comment"># Use dt.days we can get int64 data type</span></span><br><span class="line">data4[<span class="string">'daydiff'</span>]=data4[<span class="string">'daydiff'</span>].dt.days </span><br><span class="line"></span><br><span class="line">recency=data4.groupby(<span class="string">'customer_id'</span>)[<span class="string">'daydiff'</span>].agg([(<span class="string">'recency'</span>,<span class="string">'min'</span>)])</span><br><span class="line">frequency=data4.groupby(<span class="string">'customer_id'</span>)[<span class="string">'transaction_id'</span>].agg([(<span class="string">'frequency'</span>,<span class="string">'count'</span>)])</span><br><span class="line">spend=data4.groupby(<span class="string">'customer_id'</span>)[<span class="string">'list_price'</span>].agg([(<span class="string">'total spend'</span>,<span class="string">'sum'</span>)])</span><br><span class="line">RFM = recency.join(frequency).join(spend)</span><br><span class="line">RFM.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we can simply assign the R scores, F scores, and M scores to each customer </span></span><br><span class="line">according to the percentile <span class="keyword">and</span> classify our customers to four categories.</span><br><span class="line">rlabel=range(<span class="number">4</span>,<span class="number">0</span>,<span class="number">-1</span>)</span><br><span class="line">fmlabel=range(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">RFM[<span class="string">'R score'</span>]=pd.cut(RFM[<span class="string">'recency'</span>],[<span class="number">0</span>,<span class="number">18</span>,<span class="number">44</span>,<span class="number">87</span>,<span class="number">353</span>],labels=rlabel)</span><br><span class="line">RFM[<span class="string">'F score'</span>]=pd.cut(RFM[<span class="string">'frequency'</span>],[<span class="number">0</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">14</span>],labels=fmlabel)</span><br><span class="line">RFM[<span class="string">'M score'</span>]=pd.cut(RFM[<span class="string">'total spend'</span>],[<span class="number">60</span>,<span class="number">4091</span>,<span class="number">5883</span>,<span class="number">8004</span>,<span class="number">19072</span>],labels=fmlabel)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Assign the RFM value</span></span><br><span class="line"><span class="comment"># Right here we can add some weights to different score</span></span><br><span class="line"><span class="comment"># right here i got the error: “unsupported operand type(s) for *: 'Categorical' and 'float'”</span></span><br><span class="line"><span class="comment"># try to convert columns to float type data to solve this problem and doesn't work, So I change to for loop method.</span></span><br><span class="line"></span><br><span class="line">S=[]</span><br><span class="line"><span class="keyword">for</span> i,j,k <span class="keyword">in</span> zip(RFM[<span class="string">'R score'</span>],RFM[<span class="string">'F score'</span>],RFM[<span class="string">'M score'</span>]):</span><br><span class="line">    s=i*<span class="number">0.5</span>+j*<span class="number">0.8</span>+k</span><br><span class="line">    S.append(s)</span><br><span class="line"></span><br><span class="line">RFM[<span class="string">'RFM score'</span>]=S</span><br><span class="line">RFM.head()</span><br><span class="line"></span><br><span class="line"><span class="comment">## according to the RFM score distribution, we can label the customer</span></span><br><span class="line">print(RFM[<span class="string">'RFM score'</span>].describe())</span><br><span class="line">sns.distplot(RFM[<span class="string">'RFM score'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can categorize our customer to four categroies according to the RFM score percentile.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rfm_level</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> df[<span class="string">'RFM score'</span>] &gt;= <span class="number">8</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Can\'t Loose Them'</span></span><br><span class="line">    <span class="keyword">elif</span> ((df[<span class="string">'RFM score'</span>] &gt;= <span class="number">6</span>) <span class="keyword">and</span> (df[<span class="string">'RFM score'</span>] &lt; <span class="number">8</span>)):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Loyal'</span></span><br><span class="line">    <span class="keyword">elif</span> ((df[<span class="string">'RFM score'</span>] &gt;= <span class="number">4</span>) <span class="keyword">and</span> (df[<span class="string">'RFM score'</span>] &lt; <span class="number">6</span>)):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Promising'</span></span><br><span class="line">    <span class="keyword">elif</span> ((df[<span class="string">'RFM score'</span>] &gt;= <span class="number">2</span>) <span class="keyword">and</span> (df[<span class="string">'RFM score'</span>] &lt; <span class="number">4</span>)):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Needs Attention'</span></span><br><span class="line">RFM[<span class="string">'RFM level'</span>] = RFM.apply(rfm_level, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we have well defined dataframe and we can load in to Tableau for further data visualization.</span></span><br><span class="line"></span><br><span class="line">data4.to_excel(<span class="string">'C:/Users/Administrator\Desktop/kpmgdatacustomer.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Project Notes</category>
      </categories>
      <tags>
        <tag>Vitrual Internship</tag>
      </tags>
  </entry>
</search>
