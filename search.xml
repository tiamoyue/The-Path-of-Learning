<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HackerRank Problems Reviews</title>
    <url>/2020/06/04/My-second-post-to-test/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>HackerRank</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>HackerRank</tag>
        <tag>30 Days Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Big Data Specialization -- Introduction to Big Data</title>
    <url>/2020/06/04/Big%20Data/</url>
    <content><![CDATA[<h3 id="Course-1-Introduction-to-Big-Data"><a href="#Course-1-Introduction-to-Big-Data" class="headerlink" title="Course 1 Introduction to Big Data"></a>Course 1 Introduction to Big Data</h3><h4 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h4><ul>
<li><p><strong>In-Situ analytical processing:</strong> Bringing the computation to where data is located.</p>
</li>
<li><p><strong>SCADA(Supervisory Control and Data Acquisition)</strong><br>　　It is a type of industrial control system for remote monitoring and control of industrial processes that exists in the physical world, potentially including multiple sites, many types of sensors.In addition to monitoring and control, SCADA system can be used to define actions for reduced waste and improved efficiency in industrial processes,public or private infrastructure processes,facility processes.</p>
</li>
</ul>
<h4 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h4><h5 id="1-The-Characteristics-of-Big-Data-The-Six-Fundamentals-of-Big-Data"><a href="#1-The-Characteristics-of-Big-Data-The-Six-Fundamentals-of-Big-Data" class="headerlink" title="1. The Characteristics of Big Data: The Six Fundamentals of Big Data"></a><strong>1. The Characteristics of Big Data: The Six Fundamentals of Big Data</strong></h5><ol>
<li><p><strong>Volume:</strong> It’s the dimension of big data related to its size and<br>its exponential growth.</p>
</li>
<li><p><strong>Variety:</strong> refers to the ever-increasing different forms that data can come in such as text, images, voice, and geospatial data.</p>
</li>
<li><p><strong>Velocity:</strong> refers to the increasing speed at which big data is created and the increasing speed at which the data needs to be stored and analyzed.</p>
<ul>
<li><strong>Batch Processing （Slow）:</strong> Collect Data, Clean Data, Feed-In Chunks, Wait, Act. </li>
<li><strong>Real-Time Processing ( Fast):</strong> Instantly Capture Streaming Data, Feed real-time to machines, Process Real-Time, Act.</li>
</ul>
</li>
<li><p><strong>Veracity:</strong> refers to the quality of big data. It sometimes gets referred to as validity or volatility referring to the lifetime of the data.</p>
</li>
<li><p><strong>Valence (connectedness):</strong> The more connected data is, the higher it’s valences</p>
</li>
<li><p><strong>Value:</strong> The ultimate goal of big data is to get value out to the data.</p>
</li>
</ol>
<h5 id="2-Defining-the-Questions-Building-a-big-data-strategy"><a href="#2-Defining-the-Questions-Building-a-big-data-strategy" class="headerlink" title="2. Defining the Questions - Building a big data strategy:"></a><strong>2. Defining the Questions - Building a big data strategy:</strong></h5><p>As a summary, when building a big data strategy,it is important to </p>
<ul>
<li>Integrate big data analytics with business objectives</li>
<li>Communicate goals and provide organizational buy-in (Commitment, Sponsorship, Communication) for analytics projects</li>
<li>Build teams with diverse talents, and establish a teamwork mindset.</li>
<li>Remove barriers to data access and integration</li>
<li>Finally, these activities need to be iterated to respond to new business goals and technological advances</li>
</ul>
<p><strong>Five P’s of Data Science</strong></p>
<p><img src="/images/fivep.png" alt="Hadoop Ecosystem"></p>
<ol>
<li><p><strong>Purpose:</strong> The purpose refers to the challenge or set of challenges defined by your big data strategy. The purpose can be related to a scientific analysis with a hypothesis or a business metric that needs to be analyzed based often on Big Data.</p>
</li>
<li><p><strong>People:</strong> The data scientists are often seen as people who possess skills on a variety of topics including science or business domain knowledge; analysis using statistics, machine learning and mathematical knowledge; data management, programming, and computing. In practice, this is generally a group of researchers comprised of people with complementary skills.</p>
</li>
<li><p><strong>Process:</strong> The process of data science includes techniques for statistics, machine learning, programming, computing, and data management. A process is conceptual in the beginning and defines the course set of steps and how everyone can contribute to it. Note that similar reusable processes can apply to many applications with different purposes when employed within different workflows. Data science workflows combine such steps in executable graphs. We believe that process-oriented thinking is a transformative way of conducting data science to connect people and techniques to applications. Execution of such a data science process requires access to many datasets, Big and small, bringing new opportunities and challenges to Data Science. There are many Data Science steps or tasks, such as Data Collection, Data Cleaning, Data Processing/Analysis, Result Visualization, resulting in a Data Science Workflow. Data Science Processes may need user interaction and other manual operations, or be fully automated. Challenges for the data science process include 1) how to easily integrate all needed tasks to build such a process; 2) how to find the best computing resources and efficiently schedule process executions to the resources based on process definition, parameter settings, and user preferences.</p>
</li>
<li><p><strong>Platforms:</strong> Based on the needs of an application-driven purpose and the amount of data and computing required to perform this application, different computing and data platforms can be used as a part of the data science process. This scalability should be made part of any data science solution architecture.</p>
</li>
<li><p><strong>Programmability:</strong> Capturing a scalable data science process requires aid from programming languages, e.g., R, and patterns, e.g., MapReduce. Tools that provide access to such programming techniques are key to making the data science process programmable on a variety of platforms.</p>
</li>
</ol>
<h5 id="3-The-Process-of-Data-Analysis-Five-steps-activities-of-the-data-science-process"><a href="#3-The-Process-of-Data-Analysis-Five-steps-activities-of-the-data-science-process" class="headerlink" title="3. The Process of Data Analysis - Five steps activities of the data science process:"></a><strong>3. The Process of Data Analysis - Five steps activities of the data science process:</strong></h5><ol>
<li><p><strong>Acquire:</strong> includes anything that makes us retrieve data including; finding, accessing, acquiring, and moving data. It includes identification of and authenticated access to all related data. And transportation of data from sources to distributed files systems</p>
</li>
<li><p><strong>Prepare:</strong> </p>
<ul>
<li><strong>Explorer:</strong> The first step in data preparation involves looking at the data to understand its nature, what it means, its quality, and format.</li>
<li><strong>Pre-processing of data:</strong> Pre-processing includes cleaning data, sub-setting or filtering data, creating data, which programs can read and understand, such as modeling raw data into a more defined data model,or packaging it using a specific data format.</li>
</ul>
</li>
<li><p><strong>Analyze:</strong> Select the analytical technique, build models.</p>
</li>
<li><p><strong>Report:</strong> Communicating results includes evaluation of analytical results. Presenting them in a visual way, creating reports that include an assessment of results with respect to success criteria.</p>
</li>
<li><p><strong>Act:</strong> Reporting insights from analysis and determining actions from insights based on the purpose.</p>
</li>
</ol>
<h4 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h4><h5 id="1-Basic-Scalable-Concepts-Scalable-Computing-over-the-Internet"><a href="#1-Basic-Scalable-Concepts-Scalable-Computing-over-the-Internet" class="headerlink" title="1. Basic Scalable Concepts - Scalable Computing over the Internet:"></a><strong>1. Basic Scalable Concepts - Scalable Computing over the Internet:</strong></h5><ul>
<li><strong>Parallel computer:</strong>  a parallel computer is a very large number of single computing nodes with specialized capabilities connected to other networks.</li>
<li><strong>Commodity cluster:</strong> affordable parallel computers with an average number of computing nodes.They are not as powerful as traditional parallel computers and are often built out of less specialized nodes. These types of systems have a higher potential for partial failures. It is this type of distributed computing that pushed for a change towards cost-effective reliable and Fault-tolerant systems for management and analysis of big data</li>
</ul>
<h5 id="2-The-Requirements-For-Big-Data-Programming-Model"><a href="#2-The-Requirements-For-Big-Data-Programming-Model" class="headerlink" title="2. The Requirements For Big Data Programming Model:"></a><strong>2. The Requirements For Big Data Programming Model:</strong></h5><ol>
<li><strong>Support Big Data Operation:</strong><ul>
<li>Split volumes of data</li>
<li>Access data fast</li>
<li>Distribute computations to nodes</li>
</ul>
</li>
<li><strong>Handle Fault tolerance</strong><ul>
<li>Replicate data partition</li>
<li>Recover files when needs</li>
</ul>
</li>
<li><strong>Enable Adding More Racks</strong><ul>
<li>Adding new resources to more or faster data without losing performance (scaling out).</li>
<li>Optimized for specific data types</li>
</ul>
</li>
</ol>
<h5 id="3-Getting-Started-With-Hadoop"><a href="#3-Getting-Started-With-Hadoop" class="headerlink" title="3. Getting Started With Hadoop"></a><strong>3. Getting Started With Hadoop</strong></h5><p><img src="/images/eco.png" alt="Hadoop Ecosystem"></p>
<p><strong>HDFS:</strong> The Hadoop Distributed File System, a storage system for big data.It serves as the foundation for most tools in the <strong>Hadoop ecosystem</strong>.</p>
<p>It provides two capabilities that are essential for managing big data.</p>
<ul>
<li>Scalability to large data sets. </li>
<li>Reliability to cope with hardware failures. By default, HDFS maintains three copies of every block</li>
</ul>
<p><strong>Two key components of HDFS:</strong></p>
<ol>
<li><p><strong>NameNode for Metadata:</strong></p>
<ul>
<li>One namenode per cluster</li>
<li>Coordinator of HDFS cluster</li>
<li>Records the name, location in the directory hierarchy and other metadata</li>
<li>Decides which data nodes to store the contents of the file and remembers this mapping</li>
</ul>
</li>
<li><p><strong>DataNode for block storage</strong></p>
<ul>
<li><strong>Datanode</strong> Runs on each node on the cluster and is responsible for storing the file blocks</li>
<li>listens to commands from the name node for block creation, deletion, and replication. </li>
<li>Replication provides two key capabilities: <ul>
<li>Fault tolerance,  Data locality</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>YARN: The Resource Manager for Hadoop</strong><br>YARN interacts with applications and schedules resources for their use.</p>
<p><strong>Essential Gear in YARN engine:</strong></p>
<ul>
<li><strong>Resource manager:</strong> controls all the resources, and decides who gets what</li>
<li><strong>Application Manager:</strong> It negotiates resource from the Resource Manager and it talks to Node Manager to get its tasks completed</li>
<li><strong>Node Manager:</strong> operates at the machine level and is in charge of a single machine</li>
<li><strong>Container (Machine):</strong> abstract Notions that signifies a resource that is a collection of CPU memory disk network and other resources within the Compute node</li>
</ul>
<p><strong>Mapreduce:</strong> 　　</p>
<p>　　It is a big data programming model that supports all<br>the requirements of big data modeling we mentioned. It can model processing large data, split complications into different parallel tasks and make efficient use of large commodity clusters and distributed file systems.</p>
<ul>
<li>Map: Applys operation to all elements and generates key-values pairs.</li>
<li>Reduce: Summarize operation on elements and construct one output file.</li>
</ul>
<p><strong>MapReduce is bad for:</strong></p>
<ul>
<li>Frequently changing data (slow)</li>
<li>Dependent tasks</li>
<li>Interactive analysis</li>
</ul>
<p><strong>When to reconsider Hadoop:</strong></p>
<ul>
<li><p><strong>Key feature that makes problem Hadoop friendly:</strong></p>
<ul>
<li>Future anticipated data growth</li>
<li>Long term availability of data</li>
<li>Many platforms over single datastore</li>
<li>High Volume, High variety</li>
</ul>
</li>
<li><p><strong>Be careful when:</strong></p>
<ul>
<li>Small dataset</li>
<li>Task Level Parallelism ( Furuethrer analysis for which tool to use in Hadoop ecosystem)</li>
<li>Advanced algorithms (Not all algorithms are scalable in Hadoop, or reducible to one of the programming models supported by YARN</li>
<li>Random Data Access ( you may have to read an entire file just to pick one data entry)</li>
</ul>
</li>
</ul>
<h5 id="4-Cloud-Computing-On-demand-computing-it-enables-us-to-compute-any-time-any-anywhere"><a href="#4-Cloud-Computing-On-demand-computing-it-enables-us-to-compute-any-time-any-anywhere" class="headerlink" title="4. Cloud Computing( On-demand computing: it enables us to compute any time any anywhere)"></a><strong>4. Cloud Computing( On-demand computing: it enables us to compute any time any anywhere)</strong></h5><p><strong>Service Model:</strong></p>
<ul>
<li><p><strong>IaaS:</strong> infrastructure as a service, can be defined as a bare minimum rental service. This is like renting a truck from a company that you can assume has hardware and you do the packing of your furniture, and drive to your new house.</p>
</li>
<li><p><strong>PaaS:</strong> platform as a service,<br>is the model where a user is provided with an entire computing platform.<br>This could include the operating system and programming languages that you need</p>
</li>
<li><p><strong>SaaS:</strong> the software as a service model, is the model<br>in which the cloud service provider takes the responsibilities for the hardware and software environments such as the operating system and the application software</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Online Course</category>
      </categories>
      <tags>
        <tag>Coursera</tag>
        <tag>Big Data</tag>
        <tag>UC San Diego</tag>
      </tags>
  </entry>
  <entry>
    <title>KPMG Vitrual Internship Notes</title>
    <url>/2020/06/02/posttry-to-keep-my-website-runing/</url>
    <content><![CDATA[<h3 id="Task-1-Identifying-the-data-quality-issues-and-how-this-may-impact-our-analysis-going-forward"><a href="#Task-1-Identifying-the-data-quality-issues-and-how-this-may-impact-our-analysis-going-forward" class="headerlink" title="Task 1: Identifying the data quality issues and how this may impact our analysis going forward?"></a>Task 1: Identifying the data quality issues and how this may impact our analysis going forward?</h3><p>Since the company didn’t define its own business definition for data quality evaluation, I will follow the general process.(This task is intentionally solved by Python)</p>
<p>To evaluate the quality of data, we can follow the Six Standard Data Quality Dimension: </p>
<ul>
<li><strong>Accuracy</strong>: The degree to which the data correctly describe the ‘real-world’ objects. <strong>Example:</strong> <em>if a man is 30 years old, but the data is 35 years old.</em></li>
<li><strong>Completeness</strong>: Whether the data fulfill the expectation of business comprehensiveness; in other words, whether you can extract the information you want from the data.</li>
<li><strong>Uniqueness</strong>: Make sure there is no duplicate record in the dataset. <strong>Example</strong>: <em>There are 300 students in total, but there are 350 records</em></li>
<li><strong>Timeliness</strong>：Make sure that the data is recorded at the time when it occurred. No delay.</li>
<li><strong>Validity</strong>：Data are valid if it conforms to the syntax (format, type, range) of its definition.**</li>
<li><strong>Consistency</strong>：The data should be the same as input in other columns.<strong>Example</strong>: <em>If a student name:’Peter Pan’ is in the class name-list, then ‘Peter Pan’in the school name-list should be the same as class name-list.</em> </li>
</ul>
<p><a href="https://www.whitepapers.em360tech.com/wp-content/files_mf/1407250286DAMAUKDQDimensionsWhitePaperR37.pdf" target="_blank" rel="noopener">Reference: Defining Data Quality Dimensions</a></p>
<h4 id="Task-1-Problems-and-Notes"><a href="#Task-1-Problems-and-Notes" class="headerlink" title="Task 1 Problems and Notes"></a>Task 1 Problems and Notes</h4><ol>
<li>How to use python read the sheets in Excel files ?</li>
</ol>
]]></content>
      <categories>
        <category>Project Notes</category>
      </categories>
      <tags>
        <tag>Vitrual Internship</tag>
      </tags>
  </entry>
  <entry>
    <title>Web-Mining Full Project Report</title>
    <url>/2020/06/26/eb-Mining-Full-Project-Report/</url>
    <content><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>　In the past one year, PUBG became one of the most popular games in the word and at the same time it also received thousands of negative comments and thus we planned to start a project to help those developers and operators extract valuable information from those critics. The purpose of my project is to get suggestions from those critics, and then I plan to assign labels for players manually to classify those clear-minded supporters and critics and then train some classification models to discover more critics with firm stand. Next, I made a word interpretation base on those comments made by critics and made some suggestions to PUBG. Finally, I proposed some future improvements for the project.</p>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>　PUBG, one of the most popular shooting games on steam, was published by PUBG Corporation, a subsidiary of South Korean video game company Bluehole. This game was released officially in September 2018. The game is one of the best-selling of all time, with over fifty million sold across all platforms by June 2018. In addition, the Windows version holds a peak concurrent player count of over three million on Steam, which is an all-time high on the platform. PUBG received thousands of reviews from players, who found that there are still many bugs and problems existed in the game.Thus, I decide to research on discovering useful information from those reviews.</p>
<h3 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h3><p>　In this project, I am trying to detect some useful information from the reviews and have a deep understanding of the game by analysis crawled from the Steam reviews community. After this project, I hope my analysis will help game developers identify the problems that most players concerned about.</p>
<h3 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h3><p>　In this project, the data source comes from Steam.I use <strong>Selenium</strong> to scrape some reviews page. And we put all the reviews until the bottom of the page by executing Javascript. I use CSS selectors to get all the attributes including user comment date, user reviews, whether users recommend or not, how many hours spent on the game, and how many games users hold in the account.</p>
<h3 id="Data-Description"><a href="#Data-Description" class="headerlink" title="Data Description"></a>Data Description</h3><p>　The data size is 2070. The data contains three independent variables including spent hours, hold products and user reviews, and one dependent variable which is “recommended or not”. Spent hours refers to how many hours that players have spent on the game. Hold products refers to how many games user hold in their accounts. User reviews refer to what players comment on the website. Recommend or not refers to whether the player recommends this game or not. The sample data presented below:<br>On the platform, each user can easily write down their feedbacks toward the game and they will assign a label of whether they want to recommend or not-recommend this game to other players. Steam will also list the information of users such as hours of playing, the Steam products in the user’s account, and other players’ attitudes toward this comment. This review system will give the game companies or developers an overall insight into how this running on this platform by collecting all the data from users’ reviews. The user interface shows below:</p>
<h3 id="Data-cleaning"><a href="#Data-cleaning" class="headerlink" title="Data cleaning"></a>Data cleaning</h3><p>　The data contains some symbols and stop words which cause problems for me to find meaningful information from the user reviews and thus I remove these noise and transfer all characters into lower cases and split the reviews into words.</p>
<h3 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a>Exploratory Data Analysis</h3><p>　Next, I did some simple exploratory data analysis to find the correlation among these variables. Firstly, I made a bar chart to present the relation between spent hours  and recommendations<br>As the bar chart presents above, I find that the people who have spent more than 1900 hours on PUBG tend to recommend this game. In other words, this game is very popular with old players, and thus the reviews made by this part of players are more meaningful for the operator.<br>And then, I made a scatter plot to explore the relation between spent hours and hold products.</p>
<h3 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h3><p>　In this part, I use both supervised learning algorithms and unsupervised learning algorithms to make models.</p>
<h4 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h4><h5 id="VADER"><a href="#VADER" class="headerlink" title="VADER"></a>VADER</h5><p>　<strong>VADER</strong> analyzes a piece of text to see if any of the words in the text is present in the lexicon. Sentiment metrics are derived from the ratings of such words positive, neutral, and negative, represent the proportion of the text that falls into those categories.The final metric, the compound score, is the sum of all the lexicon ratings which have been standardized to range between -1 and 1 based on some heuristics.In my project, I want to see users’ sentiment for each of the reviews by applying VADER analysis, and thus group those negative sentiment for company inspect.<br>From the result shown above, reviews have been classified into three categories: positive sentiment, neutral sentiment, and negative sentiment.In positive sentiment,77.45% of reviews have been correctly　classified;In neutral sentiment,75.33 are recommended reviews and 24.66% are not recommended reviews; In negative sentiment, 26.38% negative reviews　have been correctly classified.I visualize the model result.The model has bad performance on negative sentiment. And then I try to find the reasons.After I dig into the reviews, I find out the reviews sometimes full of sarcasm, the computer is not capable of recognizing the emotion behind the sentences. For example: “This game used to be good, but now it is just game for cheaters.” Then I got a conclusion that the labels in the review system didn’t truly reflect the attitudes of players and thus the game developer can also possibly mislead by the data of labels.</p>
<h5 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h5><p>　Due to the massive information of reviews, it’s difficult for operators to analyze the reviews one by one. So, I use LDA (Latent Dirichlet allocation) to generate topics for the document. This model reduces the dimensionalities of words and thus it worked more efficiently compared with bag-of-words model and got rid of overfitting. I visualized three top topics as below:<br>Through these pictures, I found more negative words than positive words. These words state that the server of the game is bad and mountains of cheaters in this game and this game didn’t any improvement over time. This result may confuse us why there are more negative words and in the EDA stage I found that more people recommended this game, but in the VADER model, I have found that 75.33% of people who have neutral attitudes have been classified as supporters. And by reading some comments, I found that these people’s comments contain a lot of negative words and that’s the reason why I found so many negative words here. Through the LDA and VADER model, I got a conclusion that I cannot simply classify people by their comments’ labels, and thus, I planned to select those clear-minded comments to train the classification model to find more players with a firm stand.</p>
<h4 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h4><p>　The classification models are applied to detect positive and negative users. Since there are a great many reviews are not labeled in other sources. It is necessary to classify a large number of reviews and get directions for later improvement, especially from the negative sentiments.</p>
<h5 id="Multinomial-Naive-Bayes"><a href="#Multinomial-Naive-Bayes" class="headerlink" title="Multinomial Naïve Bayes"></a>Multinomial Naïve Bayes</h5><p>　<strong>Multinomial Naïve Bayes</strong> is suitable for classification with word counts for text classification, such as TF-IDF weight.<br>As the result shows above, this model has a good performance. The precision rate is 0.84 and the recall rate is 0.82.</p>
<h5 id="Support-Vector-Classification"><a href="#Support-Vector-Classification" class="headerlink" title="Support Vector Classification"></a>Support Vector Classification</h5><p>　<strong>Support vector machine</strong> constructs a hyper-plane or set of hyper-planes in a high or infinite-dimensional space, which can be used for classification. As the pictures are shown above, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class. It has 0.85 of precision rate and 0.83 of recall rate.</p>
<h5 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a><strong>Logistic Regression</strong></h5><p>　<strong>Logistic Regression</strong> model the probabilities of a recommendation as a linear function of documents term matrix and classify reviews into two categories based on TF-IDF weights of bag-of-words.<br>As the pictures are shown above, it has 84% precision rate and 80% recall rate. In summary, MNB, SVM, and Logistic Regression have good performance.</p>
<h5 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a><strong>CNN</strong></h5><p>　<strong>CNN</strong> model is introduced since it has advantages in imbalanced data classification. The doc2vec and word2vec are used to train a large number of unlabeled reviews to generate a fixed weights word matrix to map the word in the embedding phase. The doc2vec is different in predicting the word by concatenating the paragraph vector D (shared within the paragraph). The words vector trained by Doc2Vec has a better performance when checking the similarities of the words.<br>The figure describes the top 5 most similar word with ‘play’:<br>The CNN model is using three sizes of filters (bigram, trigram, and quadrigram), each size has 64 filters in the convolutional layer.<br>In summary, the CNN model that using a pre-trained matrix has a better performance as the picture shown above.</p>
<h3 id="Word-Interpretation"><a href="#Word-Interpretation" class="headerlink" title="Word Interpretation"></a>Word Interpretation</h3><p>　To get some insights from negative sentiment reviews, I use TFIDF weights and Word2Vec to dig out content from reviews.<br>As the picture shows, I picked the top ten words which have the highest frequency. Some words such as bad, time, money are keywords that might point to the potential problem.<br>Next step, I use Word2Vec to find the most similar words with these top 10 words, trying to find correlations between targeted words.<br>As a result, the problems found are as follows:</p>
<ol>
<li>A lot of game issues</li>
<li>Serious time delay for server lags</li>
<li>Devs(Developer) fix the bug</li>
<li>Vehicle issue in the game</li>
<li>Waste money</li>
<li>A lot of cheaters</li>
</ol>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>　In unsupervised learning, I found that the label in the dataset cannot accurately describe the attitude of players, but the purpose of my project is to excavate valuable information from critics and thus it’s very important to train a model to find those critics with frim stand. And then I trained several classification models including Multinomial Naïve Bayes, SVM, Logistic Regression, and CNN, and found that CNN has the best performance among these models. Finally, I explain the words meaning after classification and then give the suggestion based on what I got from the result and raise the suggestion to the company. Wipe out cheaters from the game, it hugely impacts the user’s game experience. Give more support to users when they meet with game issues. Developers need to put more effort to optimize game and fix those existed bugs. Cautiously deal with the micro-transaction, make it reasonable to users.</p>
<h3 id="Future-Improvement"><a href="#Future-Improvement" class="headerlink" title="Future Improvement"></a>Future Improvement</h3><p>　There are still some improvements that I can make in the future. For example, I can try the Steam Game Platform API,and I can improve VADER by updating the word list. Since VADER is defined as Sentiment metrics techniques, I expect to improve model performance by updating the word list and re-training the model.</p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
        <tag>Web Scraping</tag>
      </tags>
  </entry>
</search>
